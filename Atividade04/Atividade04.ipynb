{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atividade 04 -  Text Preprocessing\n",
    "\n",
    "## Davi Costa Nascimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1. Tokenization \n",
    "    - a) Generates a list of tokenized words\n",
    "- 2. Cleaning \n",
    "    - a) Lower casing \n",
    "    - b) Removal of Punctuations \n",
    "    - c) Removal of Stopwords \n",
    "    - d) Removal of Frequent words \n",
    "    - e) Removal of Rare words \n",
    "    - f) Removal of emojis \n",
    "    - g) Removal of emoticons \n",
    "    - h) Removal of URLs \n",
    "    - i) Removal of HTML tags/markups \n",
    "    - j) Chat words conversion \n",
    "    - k) Spelling correction\n",
    "- 3. Normalization \n",
    "    - a) Converting dates to text \n",
    "    - b) Numbers to text \n",
    "    - c) Currency/Percent signs to text \n",
    "    - d) Spelling mistakes correction \n",
    "    - e) Conversion of emoticons to words \n",
    "    - f) Conversion of emojis to words\n",
    "- 4. Lemmatization\n",
    "- 5. Steming\n",
    "- 6. Consider a CORPUS of texts and convert text data to vectors of numbers \n",
    "    - a) A vocabulary of known words (tokens) is extracted from the text, the occurence of words is scored, and the resulting numerical values are saved in vocabulary-long vectors. There are a few versions of BoW, corresponding to different words scoring methods.\n",
    "        - Binary (wheter the word is in the text or not)\n",
    "        - Word Counts ( number of word occurrences in the text)\n",
    "        - Term Frequencies\n",
    "        - Term Frequency-Inverse Document Frequencies\n",
    "        - bigrams e trigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ôªøa capivara (nome cient√≠fico: hydrochoerus hydrochaeris) √© uma esp√©cie de\n",
      "mam√≠fero roedor da fam√≠lia caviidae e subfam√≠lia hydrochoerinae. alguns autores\n",
      "consideram que deva ser classificada em uma fam√≠lia pr√≥pria. est√° inclu√≠da no\n",
      "mesmo grupo de roedores ao qual se classificam as pacas, cutias, os pre√°s e o\n",
      "porquinho-da-√≠ndia. ocorre por toda a am√©rica do sul ao leste dos andes em\n",
      "habitats associados a rios, lagos e p√¢ntanos, do n√≠vel do mar at√© 1 300 m de\n",
      "altitude. extremamente adapt√°vel, pode ocorrer em ambientes altamente alterados\n",
      "pelo ser humano.\n",
      "\n",
      "\n",
      "√© o maior roedor do mundo, pesando at√© 91 kg e medindo at√© 1,2 m de comprimento\n",
      "e 60 cm de altura. a pelagem √© densa, de cor avermelhada a marrom escuro. √©\n",
      "poss√≠vel distinguir os machos por conta da presen√ßa de uma gl√¢ndula proeminente\n",
      "no focinho apesar do dimorfismo sexual n√£o ser aparente. existe uma s√©rie de\n",
      "adapta√ß√µes no sistema digest√≥rio √† herbivoria, principalmente no ceco. alcan√ßa\n",
      "a maturidade sexual com cerca de 1,5 ano de idade, e as f√™meas d√£o √† luz\n",
      "geralmente a quatro filhotes por vez, pesando at√© 1,5 kg e j√° nascem com pelos\n",
      "e denti√ß√£o permanente. em cativeiro, pode viver at√© 12 anos de idade.\n"
     ]
    }
   ],
   "source": [
    "# Carregando texto\n",
    "\n",
    "with open('files/capivara-pt.txt','r') as f:\n",
    "    text = f.readlines()\n",
    "    \n",
    "text = ''.join(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Generates a list of tokenized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'capivara',\n",
       " 'nome',\n",
       " 'cient√≠fico',\n",
       " 'hydrochoerus',\n",
       " 'hydrochaeris',\n",
       " '√©',\n",
       " 'uma',\n",
       " 'esp√©cie',\n",
       " 'de',\n",
       " 'mam√≠fero',\n",
       " 'roedor',\n",
       " 'da',\n",
       " 'fam√≠lia',\n",
       " 'caviidae',\n",
       " 'e',\n",
       " 'subfam√≠lia',\n",
       " 'hydrochoerinae',\n",
       " 'alguns',\n",
       " 'autores',\n",
       " 'consideram',\n",
       " 'que',\n",
       " 'deva',\n",
       " 'ser',\n",
       " 'classificada',\n",
       " 'em',\n",
       " 'uma',\n",
       " 'fam√≠lia',\n",
       " 'pr√≥pria',\n",
       " 'est√°',\n",
       " 'inclu√≠da',\n",
       " 'no',\n",
       " 'mesmo',\n",
       " 'grupo',\n",
       " 'de',\n",
       " 'roedores',\n",
       " 'ao',\n",
       " 'qual',\n",
       " 'se',\n",
       " 'classificam',\n",
       " 'as',\n",
       " 'pacas',\n",
       " 'cutias',\n",
       " 'os',\n",
       " 'pre√°s',\n",
       " 'e',\n",
       " 'o',\n",
       " 'porquinho',\n",
       " 'da',\n",
       " '√≠ndia',\n",
       " 'ocorre',\n",
       " 'por',\n",
       " 'toda',\n",
       " 'a',\n",
       " 'am√©rica',\n",
       " 'do',\n",
       " 'sul',\n",
       " 'ao',\n",
       " 'leste',\n",
       " 'dos',\n",
       " 'andes',\n",
       " 'em',\n",
       " 'habitats',\n",
       " 'associados',\n",
       " 'a',\n",
       " 'rios',\n",
       " 'lagos',\n",
       " 'e',\n",
       " 'p√¢ntanos',\n",
       " 'do',\n",
       " 'n√≠vel',\n",
       " 'do',\n",
       " 'mar',\n",
       " 'at√©',\n",
       " '1',\n",
       " '300',\n",
       " 'm',\n",
       " 'de',\n",
       " 'altitude',\n",
       " 'extremamente',\n",
       " 'adapt√°vel',\n",
       " 'pode',\n",
       " 'ocorrer',\n",
       " 'em',\n",
       " 'ambientes',\n",
       " 'altamente',\n",
       " 'alterados',\n",
       " 'pelo',\n",
       " 'ser',\n",
       " 'humano',\n",
       " '√©',\n",
       " 'o',\n",
       " 'maior',\n",
       " 'roedor',\n",
       " 'do',\n",
       " 'mundo',\n",
       " 'pesando',\n",
       " 'at√©',\n",
       " '91',\n",
       " 'kg',\n",
       " 'e',\n",
       " 'medindo',\n",
       " 'at√©',\n",
       " '1',\n",
       " '2',\n",
       " 'm',\n",
       " 'de',\n",
       " 'comprimento',\n",
       " 'e',\n",
       " '60',\n",
       " 'cm',\n",
       " 'de',\n",
       " 'altura',\n",
       " 'a',\n",
       " 'pelagem',\n",
       " '√©',\n",
       " 'densa',\n",
       " 'de',\n",
       " 'cor',\n",
       " 'avermelhada',\n",
       " 'a',\n",
       " 'marrom',\n",
       " 'escuro',\n",
       " '√©',\n",
       " 'poss√≠vel',\n",
       " 'distinguir',\n",
       " 'os',\n",
       " 'machos',\n",
       " 'por',\n",
       " 'conta',\n",
       " 'da',\n",
       " 'presen√ßa',\n",
       " 'de',\n",
       " 'uma',\n",
       " 'gl√¢ndula',\n",
       " 'proeminente',\n",
       " 'no',\n",
       " 'focinho',\n",
       " 'apesar',\n",
       " 'do',\n",
       " 'dimorfismo',\n",
       " 'sexual',\n",
       " 'n√£o',\n",
       " 'ser',\n",
       " 'aparente',\n",
       " 'existe',\n",
       " 'uma',\n",
       " 's√©rie',\n",
       " 'de',\n",
       " 'adapta√ß√µes',\n",
       " 'no',\n",
       " 'sistema',\n",
       " 'digest√≥rio',\n",
       " '√†',\n",
       " 'herbivoria',\n",
       " 'principalmente',\n",
       " 'no',\n",
       " 'ceco',\n",
       " 'alcan√ßa',\n",
       " 'a',\n",
       " 'maturidade',\n",
       " 'sexual',\n",
       " 'com',\n",
       " 'cerca',\n",
       " 'de',\n",
       " '1',\n",
       " '5',\n",
       " 'ano',\n",
       " 'de',\n",
       " 'idade',\n",
       " 'e',\n",
       " 'as',\n",
       " 'f√™meas',\n",
       " 'd√£o',\n",
       " '√†',\n",
       " 'luz',\n",
       " 'geralmente',\n",
       " 'a',\n",
       " 'quatro',\n",
       " 'filhotes',\n",
       " 'por',\n",
       " 'vez',\n",
       " 'pesando',\n",
       " 'at√©',\n",
       " '1',\n",
       " '5',\n",
       " 'kg',\n",
       " 'e',\n",
       " 'j√°',\n",
       " 'nascem',\n",
       " 'com',\n",
       " 'pelos',\n",
       " 'e',\n",
       " 'denti√ß√£o',\n",
       " 'permanente',\n",
       " 'em',\n",
       " 'cativeiro',\n",
       " 'pode',\n",
       " 'viver',\n",
       " 'at√©',\n",
       " '12',\n",
       " 'anos',\n",
       " 'de',\n",
       " 'idade']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "# Aplicando Tokeniza√ß√£o por palavras\n",
    "token_list = tokenizer.tokenize(text)\n",
    "\n",
    "token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Lower casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'capivara',\n",
       " 'nome',\n",
       " 'cient√≠fico',\n",
       " 'hydrochoerus',\n",
       " 'hydrochaeris',\n",
       " '√©',\n",
       " 'uma',\n",
       " 'esp√©cie',\n",
       " 'de',\n",
       " 'mam√≠fero',\n",
       " 'roedor',\n",
       " 'da',\n",
       " 'fam√≠lia',\n",
       " 'caviidae',\n",
       " 'e',\n",
       " 'subfam√≠lia',\n",
       " 'hydrochoerinae',\n",
       " 'alguns',\n",
       " 'autores',\n",
       " 'consideram',\n",
       " 'que',\n",
       " 'deva',\n",
       " 'ser',\n",
       " 'classificada',\n",
       " 'em',\n",
       " 'uma',\n",
       " 'fam√≠lia',\n",
       " 'pr√≥pria',\n",
       " 'est√°',\n",
       " 'inclu√≠da',\n",
       " 'no',\n",
       " 'mesmo',\n",
       " 'grupo',\n",
       " 'de',\n",
       " 'roedores',\n",
       " 'ao',\n",
       " 'qual',\n",
       " 'se',\n",
       " 'classificam',\n",
       " 'as',\n",
       " 'pacas',\n",
       " 'cutias',\n",
       " 'os',\n",
       " 'pre√°s',\n",
       " 'e',\n",
       " 'o',\n",
       " 'porquinho',\n",
       " 'da',\n",
       " '√≠ndia',\n",
       " 'ocorre',\n",
       " 'por',\n",
       " 'toda',\n",
       " 'a',\n",
       " 'am√©rica',\n",
       " 'do',\n",
       " 'sul',\n",
       " 'ao',\n",
       " 'leste',\n",
       " 'dos',\n",
       " 'andes',\n",
       " 'em',\n",
       " 'habitats',\n",
       " 'associados',\n",
       " 'a',\n",
       " 'rios',\n",
       " 'lagos',\n",
       " 'e',\n",
       " 'p√¢ntanos',\n",
       " 'do',\n",
       " 'n√≠vel',\n",
       " 'do',\n",
       " 'mar',\n",
       " 'at√©',\n",
       " '1',\n",
       " '300',\n",
       " 'm',\n",
       " 'de',\n",
       " 'altitude',\n",
       " 'extremamente',\n",
       " 'adapt√°vel',\n",
       " 'pode',\n",
       " 'ocorrer',\n",
       " 'em',\n",
       " 'ambientes',\n",
       " 'altamente',\n",
       " 'alterados',\n",
       " 'pelo',\n",
       " 'ser',\n",
       " 'humano',\n",
       " '√©',\n",
       " 'o',\n",
       " 'maior',\n",
       " 'roedor',\n",
       " 'do',\n",
       " 'mundo',\n",
       " 'pesando',\n",
       " 'at√©',\n",
       " '91',\n",
       " 'kg',\n",
       " 'e',\n",
       " 'medindo',\n",
       " 'at√©',\n",
       " '1',\n",
       " '2',\n",
       " 'm',\n",
       " 'de',\n",
       " 'comprimento',\n",
       " 'e',\n",
       " '60',\n",
       " 'cm',\n",
       " 'de',\n",
       " 'altura',\n",
       " 'a',\n",
       " 'pelagem',\n",
       " '√©',\n",
       " 'densa',\n",
       " 'de',\n",
       " 'cor',\n",
       " 'avermelhada',\n",
       " 'a',\n",
       " 'marrom',\n",
       " 'escuro',\n",
       " '√©',\n",
       " 'poss√≠vel',\n",
       " 'distinguir',\n",
       " 'os',\n",
       " 'machos',\n",
       " 'por',\n",
       " 'conta',\n",
       " 'da',\n",
       " 'presen√ßa',\n",
       " 'de',\n",
       " 'uma',\n",
       " 'gl√¢ndula',\n",
       " 'proeminente',\n",
       " 'no',\n",
       " 'focinho',\n",
       " 'apesar',\n",
       " 'do',\n",
       " 'dimorfismo',\n",
       " 'sexual',\n",
       " 'n√£o',\n",
       " 'ser',\n",
       " 'aparente',\n",
       " 'existe',\n",
       " 'uma',\n",
       " 's√©rie',\n",
       " 'de',\n",
       " 'adapta√ß√µes',\n",
       " 'no',\n",
       " 'sistema',\n",
       " 'digest√≥rio',\n",
       " '√†',\n",
       " 'herbivoria',\n",
       " 'principalmente',\n",
       " 'no',\n",
       " 'ceco',\n",
       " 'alcan√ßa',\n",
       " 'a',\n",
       " 'maturidade',\n",
       " 'sexual',\n",
       " 'com',\n",
       " 'cerca',\n",
       " 'de',\n",
       " '1',\n",
       " '5',\n",
       " 'ano',\n",
       " 'de',\n",
       " 'idade',\n",
       " 'e',\n",
       " 'as',\n",
       " 'f√™meas',\n",
       " 'd√£o',\n",
       " '√†',\n",
       " 'luz',\n",
       " 'geralmente',\n",
       " 'a',\n",
       " 'quatro',\n",
       " 'filhotes',\n",
       " 'por',\n",
       " 'vez',\n",
       " 'pesando',\n",
       " 'at√©',\n",
       " '1',\n",
       " '5',\n",
       " 'kg',\n",
       " 'e',\n",
       " 'j√°',\n",
       " 'nascem',\n",
       " 'com',\n",
       " 'pelos',\n",
       " 'e',\n",
       " 'denti√ß√£o',\n",
       " 'permanente',\n",
       " 'em',\n",
       " 'cativeiro',\n",
       " 'pode',\n",
       " 'viver',\n",
       " 'at√©',\n",
       " '12',\n",
       " 'anos',\n",
       " 'de',\n",
       " 'idade']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_token = [i.lower()for i in token_list]\n",
    "lower_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\tRemoval of Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Isso √© bom    Isso √© muito    '"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str = \"Isso √© bom ?: Isso √© muito ! ;\"\n",
    "punc = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''\n",
    "\n",
    "for ele in test_str:  \n",
    "    if ele in punc:  \n",
    "        test_str = test_str.replace(ele, \" \")\n",
    "\n",
    "test_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  c)\tRemoval of Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/davi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['capivara',\n",
       " 'nome',\n",
       " 'cient√≠fico',\n",
       " 'hydrochoerus',\n",
       " 'hydrochaeris',\n",
       " 'esp√©cie',\n",
       " 'mam√≠fero',\n",
       " 'roedor',\n",
       " 'fam√≠lia',\n",
       " 'caviidae',\n",
       " 'subfam√≠lia',\n",
       " 'hydrochoerinae',\n",
       " 'alguns',\n",
       " 'autores',\n",
       " 'consideram',\n",
       " 'deva',\n",
       " 'ser',\n",
       " 'classificada',\n",
       " 'fam√≠lia',\n",
       " 'pr√≥pria',\n",
       " 'inclu√≠da',\n",
       " 'grupo',\n",
       " 'roedores',\n",
       " 'classificam',\n",
       " 'pacas',\n",
       " 'cutias',\n",
       " 'pre√°s',\n",
       " 'porquinho',\n",
       " '√≠ndia',\n",
       " 'ocorre',\n",
       " 'toda',\n",
       " 'am√©rica',\n",
       " 'sul',\n",
       " 'leste',\n",
       " 'andes',\n",
       " 'habitats',\n",
       " 'associados',\n",
       " 'rios',\n",
       " 'lagos',\n",
       " 'p√¢ntanos',\n",
       " 'n√≠vel',\n",
       " 'mar',\n",
       " '1',\n",
       " '300',\n",
       " 'm',\n",
       " 'altitude',\n",
       " 'extremamente',\n",
       " 'adapt√°vel',\n",
       " 'pode',\n",
       " 'ocorrer',\n",
       " 'ambientes',\n",
       " 'altamente',\n",
       " 'alterados',\n",
       " 'ser',\n",
       " 'humano',\n",
       " 'maior',\n",
       " 'roedor',\n",
       " 'mundo',\n",
       " 'pesando',\n",
       " '91',\n",
       " 'kg',\n",
       " 'medindo',\n",
       " '1',\n",
       " '2',\n",
       " 'm',\n",
       " 'comprimento',\n",
       " '60',\n",
       " 'cm',\n",
       " 'altura',\n",
       " 'pelagem',\n",
       " 'densa',\n",
       " 'cor',\n",
       " 'avermelhada',\n",
       " 'marrom',\n",
       " 'escuro',\n",
       " 'poss√≠vel',\n",
       " 'distinguir',\n",
       " 'machos',\n",
       " 'conta',\n",
       " 'presen√ßa',\n",
       " 'gl√¢ndula',\n",
       " 'proeminente',\n",
       " 'focinho',\n",
       " 'apesar',\n",
       " 'dimorfismo',\n",
       " 'sexual',\n",
       " 'ser',\n",
       " 'aparente',\n",
       " 'existe',\n",
       " 's√©rie',\n",
       " 'adapta√ß√µes',\n",
       " 'sistema',\n",
       " 'digest√≥rio',\n",
       " 'herbivoria',\n",
       " 'principalmente',\n",
       " 'ceco',\n",
       " 'alcan√ßa',\n",
       " 'maturidade',\n",
       " 'sexual',\n",
       " 'cerca',\n",
       " '1',\n",
       " '5',\n",
       " 'ano',\n",
       " 'idade',\n",
       " 'f√™meas',\n",
       " 'd√£o',\n",
       " 'luz',\n",
       " 'geralmente',\n",
       " 'quatro',\n",
       " 'filhotes',\n",
       " 'vez',\n",
       " 'pesando',\n",
       " '1',\n",
       " '5',\n",
       " 'kg',\n",
       " 'nascem',\n",
       " 'denti√ß√£o',\n",
       " 'permanente',\n",
       " 'cativeiro',\n",
       " 'pode',\n",
       " 'viver',\n",
       " '12',\n",
       " 'anos',\n",
       " 'idade']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download de stopworks em portugu√™s\n",
    "nltk.download('stopwords')\n",
    "stop_works = set(stopwords.words('portuguese'))\n",
    "\n",
    "filter_words = [i for i in token_list if not i in stop_works]\n",
    "\n",
    "# lista de palavras sem stopwroks\n",
    "filter_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   d)\tRemoval of Frequent words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   e)\tRemoval of Rare words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'nome',\n",
       " 'cient√≠fico',\n",
       " 'hydrochoerus',\n",
       " 'hydrochaeris',\n",
       " '√©',\n",
       " 'uma',\n",
       " 'esp√©cie',\n",
       " 'de',\n",
       " 'mam√≠fero',\n",
       " 'roedor',\n",
       " 'da',\n",
       " 'fam√≠lia',\n",
       " 'caviidae',\n",
       " 'e',\n",
       " 'subfam√≠lia',\n",
       " 'hydrochoerinae',\n",
       " 'alguns',\n",
       " 'autores',\n",
       " 'consideram',\n",
       " 'que',\n",
       " 'deva',\n",
       " 'ser',\n",
       " 'classificada',\n",
       " 'em',\n",
       " 'uma',\n",
       " 'fam√≠lia',\n",
       " 'pr√≥pria',\n",
       " 'est√°',\n",
       " 'inclu√≠da',\n",
       " 'no',\n",
       " 'mesmo',\n",
       " 'grupo',\n",
       " 'de',\n",
       " 'roedores',\n",
       " 'ao',\n",
       " 'qual',\n",
       " 'se',\n",
       " 'classificam',\n",
       " 'as',\n",
       " 'pacas',\n",
       " 'cutias',\n",
       " 'os',\n",
       " 'pre√°s',\n",
       " 'e',\n",
       " 'o',\n",
       " 'porquinho',\n",
       " 'da',\n",
       " '√≠ndia',\n",
       " 'ocorre',\n",
       " 'por',\n",
       " 'toda',\n",
       " 'a',\n",
       " 'am√©rica',\n",
       " 'do',\n",
       " 'sul',\n",
       " 'ao',\n",
       " 'leste',\n",
       " 'dos',\n",
       " 'andes',\n",
       " 'em',\n",
       " 'habitats',\n",
       " 'associados',\n",
       " 'a',\n",
       " 'rios',\n",
       " 'lagos',\n",
       " 'e',\n",
       " 'p√¢ntanos',\n",
       " 'do',\n",
       " 'n√≠vel',\n",
       " 'do',\n",
       " 'mar',\n",
       " 'at√©',\n",
       " '1',\n",
       " '300',\n",
       " 'm',\n",
       " 'de',\n",
       " 'altitude',\n",
       " 'extremamente',\n",
       " 'adapt√°vel',\n",
       " 'pode',\n",
       " 'ocorrer',\n",
       " 'em',\n",
       " 'ambientes',\n",
       " 'altamente',\n",
       " 'alterados',\n",
       " 'pelo',\n",
       " 'ser',\n",
       " 'humano',\n",
       " '√©',\n",
       " 'o',\n",
       " 'maior',\n",
       " 'roedor',\n",
       " 'do',\n",
       " 'mundo',\n",
       " 'pesando',\n",
       " 'at√©',\n",
       " '91',\n",
       " 'kg',\n",
       " 'e',\n",
       " 'medindo',\n",
       " 'at√©',\n",
       " '1',\n",
       " '2',\n",
       " 'm',\n",
       " 'de',\n",
       " 'comprimento',\n",
       " 'e',\n",
       " '60',\n",
       " 'cm',\n",
       " 'de',\n",
       " 'altura',\n",
       " 'a',\n",
       " 'pelagem',\n",
       " '√©',\n",
       " 'densa',\n",
       " 'de',\n",
       " 'cor',\n",
       " 'avermelhada',\n",
       " 'a',\n",
       " 'marrom',\n",
       " 'escuro',\n",
       " '√©',\n",
       " 'poss√≠vel',\n",
       " 'distinguir',\n",
       " 'os',\n",
       " 'machos',\n",
       " 'por',\n",
       " 'conta',\n",
       " 'da',\n",
       " 'presen√ßa',\n",
       " 'de',\n",
       " 'uma',\n",
       " 'gl√¢ndula',\n",
       " 'proeminente',\n",
       " 'no',\n",
       " 'focinho',\n",
       " 'apesar',\n",
       " 'do',\n",
       " 'dimorfismo',\n",
       " 'sexual',\n",
       " 'n√£o',\n",
       " 'ser',\n",
       " 'aparente',\n",
       " 'existe',\n",
       " 'uma',\n",
       " 's√©rie',\n",
       " 'de',\n",
       " 'adapta√ß√µes',\n",
       " 'no',\n",
       " 'sistema',\n",
       " 'digest√≥rio',\n",
       " '√†',\n",
       " 'herbivoria',\n",
       " 'principalmente',\n",
       " 'no',\n",
       " 'ceco',\n",
       " 'alcan√ßa',\n",
       " 'a',\n",
       " 'maturidade',\n",
       " 'sexual',\n",
       " 'com',\n",
       " 'cerca',\n",
       " 'de',\n",
       " '1',\n",
       " '5',\n",
       " 'ano',\n",
       " 'de',\n",
       " 'idade',\n",
       " 'e',\n",
       " 'as',\n",
       " 'f√™meas',\n",
       " 'd√£o',\n",
       " '√†',\n",
       " 'luz',\n",
       " 'geralmente',\n",
       " 'a',\n",
       " 'quatro',\n",
       " 'filhotes',\n",
       " 'por',\n",
       " 'vez',\n",
       " 'pesando',\n",
       " 'at√©',\n",
       " '1',\n",
       " '5',\n",
       " 'kg',\n",
       " 'e',\n",
       " 'j√°',\n",
       " 'nascem',\n",
       " 'com',\n",
       " 'pelos',\n",
       " 'e',\n",
       " 'denti√ß√£o',\n",
       " 'permanente',\n",
       " 'em',\n",
       " 'cativeiro',\n",
       " 'pode',\n",
       " 'viver',\n",
       " 'at√©',\n",
       " '12',\n",
       " 'anos',\n",
       " 'de',\n",
       " 'idade']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list.remove('capivara')\n",
    "token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   f)\tRemoval of emojis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Um  üòÇ texto üòé com üòí emojis\n",
      "Um   texto  com  emojis\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def emoji(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", \n",
    "    flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "text = \"Um  üòÇ texto üòé com üòí emojis\"\n",
    "\n",
    "# Texto com emojis\n",
    "print(text)\n",
    "# Texto sem emojis\n",
    "print(emoji(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   g)\tRemoval of emoticons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   h)\tRemoval of URLs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Os dados est√£o dispon√≠veis em: https://physionet.org/content/adfecgdb/1.0.0/\n",
      "Os dados est√£o dispon√≠veis em: \n"
     ]
    }
   ],
   "source": [
    "url = 'https://physionet.org/content/adfecgdb/1.0.0/'\n",
    "texto = 'Os dados est√£o dispon√≠veis em: ' + url\n",
    "\n",
    "# texto com url\n",
    "print(texto)\n",
    "\n",
    "new_texto = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "# texto sem url\n",
    "print(new_texto.sub(r\"\",texto))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   i)\tRemoval of HTML tags/markups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "\n",
      "    <head>\n",
      "        <meta charset=\"UTF-8\">\n",
      "        <title>Meu t√≠tulo</title>\n",
      "    </head>\n",
      "\n",
      "    <body>\n",
      "        <h1>Meu texto H1</h1>\n",
      "        <p>Meu par√°grafo</p>\n",
      "    </body>\n",
      "\n",
      "</html>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "        \n",
      "        Meu t√≠tulo\n",
      "    \n",
      "\n",
      "    \n",
      "        Meu texto H1\n",
      "        Meu par√°grafo\n",
      "    \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "html_text = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Meu t√≠tulo</title>\n",
    "    </head>\n",
    "\n",
    "    <body>\n",
    "        <h1>Meu texto H1</h1>\n",
    "        <p>Meu par√°grafo</p>\n",
    "    </body>\n",
    "\n",
    "</html>\"\"\"\n",
    "\n",
    "# texto com HTML\n",
    "print(html_text)\n",
    "\n",
    "new_html_text = re.compile(r\"<.*?>\")\n",
    "\n",
    "# texto sem HTML\n",
    "print(new_html_text.sub(r\"\",html_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   j)\tChat words conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   k)\tSpelling correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\tNormalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  a)\tConverting dates to text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-18\n",
      "2020-October-18\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "mydate = datetime.datetime.today()\n",
    "\n",
    "print(mydate.strftime('%Y-%m-%d'))\n",
    "\n",
    "print(mydate.strftime(\"%Y\")+\"-\"+mydate.strftime(\"%B\")+\"-\"+mydate.strftime(\"%d\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   b)\tNumbers to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ten'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install inflect\n",
    "\n",
    "import inflect\n",
    "\n",
    "N = 10\n",
    "\n",
    "ie = inflect.engine()\n",
    "num_text = ie.number_to_words(10)\n",
    "num_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   c)\tCurrency/Percent signs to text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O pre√ßo era $30,55, mas houve 25% de desconto\n",
      "O pre√ßo era reais 30,55, mas houve 25 por cento de desconto\n"
     ]
    }
   ],
   "source": [
    "money_text = 'O pre√ßo era $30,55, mas houve 25% de desconto'\n",
    "\n",
    "\n",
    "# texto com \n",
    "print(money_text)\n",
    "\n",
    "# texto sem \n",
    "print(money_text.replace('$','reais ').replace('%',' por cento'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   d)\tSpelling mistakes correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   e)\tConversion of emoticons to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   f)\tConversion of emojis to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estudar NLP √© :thumbs_up:\n"
     ]
    }
   ],
   "source": [
    "# !pip install emoji\n",
    "\n",
    "import emoji\n",
    "\n",
    "print(emoji.demojize('Estudar NLP √© üëç'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\tLemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I  -  I\n",
      "'m  -  'm\n",
      "always  -  always\n",
      "learning  -  learning\n",
      "and  -  and\n",
      "looking  -  looking\n",
      "for  -  for\n",
      "news  -  news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/davi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "# word_data = \"√â preciso se comportar bem\"\n",
    "word_data = \"I'm always learning and looking for news\"\n",
    "\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "for w in nltk_tokens:\n",
    "    print(w,\" - \",wl.lemmatize(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\tSteming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I  -  I\n",
      "'m  -  'm\n",
      "always  -  alway\n",
      "learning  -  learn\n",
      "and  -  and\n",
      "looking  -  look\n",
      "for  -  for\n",
      "news  -  news\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "word_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "word_data = \"I'm always learning and looking for news\"\n",
    "\n",
    "# First Word tokenization\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "#Next find the roots of the word\n",
    "for w in nltk_tokens:\n",
    "       print(w,\" - \",porter_stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Consider a CORPUS of texts and convert text data to vectors of numbers\n",
    "\n",
    "### a) A vocabulary of known words (tokens) is extracted from the text, the occurence of words is scored, and the resulting numerical values are saved in vocabulary-long vectors. There are a few versions of BoW, corresponding to different words scoring methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "My fellow citizens:\n",
      "\n",
      "I stand here today humbled by the task before us, grateful for the trust you have bestowed, mindful of the sacrifices borne by our ancestors. I thank President Bush for his service to our nation, as well as the generosity and cooperation he has shown throughout this transition. \n",
      "Forty-four Americans have now taken the presidential oath. The words have been spoken during rising tides of prosperity and the still waters of peace. Yet, every so often the oath is taken amidst gathering clouds and raging storms. At these moments, America has carried on not simply because of the skill or vision of those in high office, but because We the People have remained faithful to the ideals of our forbearers, and true to our founding documents.\n",
      "\n",
      "So it has been. So it must be with this generation of Americans.\n",
      "\n",
      "That we are in the midst of crisis is now well understood. Our nation is at war, against a far-reaching network of violence and hatred. Our economy is badly weakened, a consequence of greed and irresponsibility on the part of some, but also our collective failure to make hard choices and prepare the nation for a new age. Homes have been lost; jobs shed; businesses shuttered. Our health care is too costly; our schools fail too many; and each day brings further evidence that the ways we use energy strengthen our adversaries and threaten our planet.\n",
      "These are the indicators of crisis, subject to data and statistics. Less measurable but no less profound is a sapping of confidence across our land - a nagging fear that America's decline is inevitable, and that the next generation must lower its sights.\n",
      "Today I say to you that the challenges we face are real. They are serious and they are many. They will not be met easily or in a short span of time. But know this, America - they will be met.\n",
      "On this day, we gather because we have chosen hope over fear, unity of purpose over conflict and discord.\n",
      "On this day, we come to proclaim an end to the petty grievances and false promises, the recriminations and worn out dogmas, that for far too long have strangled our politics.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "obama_text = \"\"\"\n",
    "My fellow citizens:\n",
    "\n",
    "I stand here today humbled by the task before us, grateful for the trust you have bestowed, mindful of the sacrifices borne by our ancestors. I thank President Bush for his service to our nation, as well as the generosity and cooperation he has shown throughout this transition. \n",
    "Forty-four Americans have now taken the presidential oath. The words have been spoken during rising tides of prosperity and the still waters of peace. Yet, every so often the oath is taken amidst gathering clouds and raging storms. At these moments, America has carried on not simply because of the skill or vision of those in high office, but because We the People have remained faithful to the ideals of our forbearers, and true to our founding documents.\n",
    "\n",
    "So it has been. So it must be with this generation of Americans.\n",
    "\n",
    "That we are in the midst of crisis is now well understood. Our nation is at war, against a far-reaching network of violence and hatred. Our economy is badly weakened, a consequence of greed and irresponsibility on the part of some, but also our collective failure to make hard choices and prepare the nation for a new age. Homes have been lost; jobs shed; businesses shuttered. Our health care is too costly; our schools fail too many; and each day brings further evidence that the ways we use energy strengthen our adversaries and threaten our planet.\n",
    "These are the indicators of crisis, subject to data and statistics. Less measurable but no less profound is a sapping of confidence across our land - a nagging fear that America's decline is inevitable, and that the next generation must lower its sights.\n",
    "Today I say to you that the challenges we face are real. They are serious and they are many. They will not be met easily or in a short span of time. But know this, America - they will be met.\n",
    "On this day, we gather because we have chosen hope over fear, unity of purpose over conflict and discord.\n",
    "On this day, we come to proclaim an end to the petty grievances and false promises, the recriminations and worn out dogmas, that for far too long have strangled our politics.\n",
    "\"\"\"\n",
    "print(obama_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Binary (wheter the word is in the text or not)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Word Counts ( number of word occurrences in the text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('America', 3)\n",
      "('Americans', 2)\n",
      "('At', 1)\n",
      "('Bush', 1)\n",
      "('But', 1)\n",
      "('Forty', 1)\n",
      "('Homes', 1)\n",
      "('I', 3)\n",
      "('Less', 1)\n",
      "('My', 1)\n",
      "('On', 2)\n",
      "('Our', 3)\n",
      "('People', 1)\n",
      "('President', 1)\n",
      "('So', 2)\n",
      "('That', 1)\n",
      "('The', 1)\n",
      "('These', 1)\n",
      "('They', 2)\n",
      "('Today', 1)\n",
      "('We', 1)\n",
      "('Yet', 1)\n",
      "('a', 6)\n",
      "('across', 1)\n",
      "('adversaries', 1)\n",
      "('against', 1)\n",
      "('age', 1)\n",
      "('also', 1)\n",
      "('amidst', 1)\n",
      "('an', 1)\n",
      "('ancestors', 1)\n",
      "('and', 15)\n",
      "('are', 5)\n",
      "('as', 2)\n",
      "('at', 1)\n",
      "('badly', 1)\n",
      "('be', 3)\n",
      "('because', 3)\n",
      "('been', 3)\n",
      "('before', 1)\n",
      "('bestowed', 1)\n",
      "('borne', 1)\n",
      "('brings', 1)\n",
      "('businesses', 1)\n",
      "('but', 3)\n",
      "('by', 2)\n",
      "('care', 1)\n",
      "('carried', 1)\n",
      "('challenges', 1)\n",
      "('choices', 1)\n",
      "('chosen', 1)\n",
      "('citizens', 1)\n",
      "('clouds', 1)\n",
      "('collective', 1)\n",
      "('come', 1)\n",
      "('confidence', 1)\n",
      "('conflict', 1)\n",
      "('consequence', 1)\n",
      "('cooperation', 1)\n",
      "('costly', 1)\n",
      "('crisis', 2)\n",
      "('data', 1)\n",
      "('day', 3)\n",
      "('decline', 1)\n",
      "('discord', 1)\n",
      "('documents', 1)\n",
      "('dogmas', 1)\n",
      "('during', 1)\n",
      "('each', 1)\n",
      "('easily', 1)\n",
      "('economy', 1)\n",
      "('end', 1)\n",
      "('energy', 1)\n",
      "('every', 1)\n",
      "('evidence', 1)\n",
      "('face', 1)\n",
      "('fail', 1)\n",
      "('failure', 1)\n",
      "('faithful', 1)\n",
      "('false', 1)\n",
      "('far', 2)\n",
      "('fear', 2)\n",
      "('fellow', 1)\n",
      "('for', 4)\n",
      "('forbearers', 1)\n",
      "('founding', 1)\n",
      "('four', 1)\n",
      "('further', 1)\n",
      "('gather', 1)\n",
      "('gathering', 1)\n",
      "('generation', 2)\n",
      "('generosity', 1)\n",
      "('grateful', 1)\n",
      "('greed', 1)\n",
      "('grievances', 1)\n",
      "('hard', 1)\n",
      "('has', 3)\n",
      "('hatred', 1)\n",
      "('have', 7)\n",
      "('he', 1)\n",
      "('health', 1)\n",
      "('here', 1)\n",
      "('high', 1)\n",
      "('his', 1)\n",
      "('hope', 1)\n",
      "('humbled', 1)\n",
      "('ideals', 1)\n",
      "('in', 3)\n",
      "('indicators', 1)\n",
      "('inevitable', 1)\n",
      "('irresponsibility', 1)\n",
      "('is', 7)\n",
      "('it', 2)\n",
      "('its', 1)\n",
      "('jobs', 1)\n",
      "('know', 1)\n",
      "('land', 1)\n",
      "('less', 1)\n",
      "('long', 1)\n",
      "('lost', 1)\n",
      "('lower', 1)\n",
      "('make', 1)\n",
      "('many', 2)\n",
      "('measurable', 1)\n",
      "('met', 2)\n",
      "('midst', 1)\n",
      "('mindful', 1)\n",
      "('moments', 1)\n",
      "('must', 2)\n",
      "('nagging', 1)\n",
      "('nation', 3)\n",
      "('network', 1)\n",
      "('new', 1)\n",
      "('next', 1)\n",
      "('no', 1)\n",
      "('not', 2)\n",
      "('now', 2)\n",
      "('oath', 2)\n",
      "('of', 15)\n",
      "('office', 1)\n",
      "('often', 1)\n",
      "('on', 2)\n",
      "('or', 2)\n",
      "('our', 10)\n",
      "('out', 1)\n",
      "('over', 2)\n",
      "('part', 1)\n",
      "('peace', 1)\n",
      "('petty', 1)\n",
      "('planet', 1)\n",
      "('politics', 1)\n",
      "('prepare', 1)\n",
      "('presidential', 1)\n",
      "('proclaim', 1)\n",
      "('profound', 1)\n",
      "('promises', 1)\n",
      "('prosperity', 1)\n",
      "('purpose', 1)\n",
      "('raging', 1)\n",
      "('reaching', 1)\n",
      "('real', 1)\n",
      "('recriminations', 1)\n",
      "('remained', 1)\n",
      "('rising', 1)\n",
      "('s', 1)\n",
      "('sacrifices', 1)\n",
      "('sapping', 1)\n",
      "('say', 1)\n",
      "('schools', 1)\n",
      "('serious', 1)\n",
      "('service', 1)\n",
      "('shed', 1)\n",
      "('short', 1)\n",
      "('shown', 1)\n",
      "('shuttered', 1)\n",
      "('sights', 1)\n",
      "('simply', 1)\n",
      "('skill', 1)\n",
      "('so', 1)\n",
      "('some', 1)\n",
      "('span', 1)\n",
      "('spoken', 1)\n",
      "('stand', 1)\n",
      "('statistics', 1)\n",
      "('still', 1)\n",
      "('storms', 1)\n",
      "('strangled', 1)\n",
      "('strengthen', 1)\n",
      "('subject', 1)\n",
      "('taken', 2)\n",
      "('task', 1)\n",
      "('thank', 1)\n",
      "('that', 5)\n",
      "('the', 19)\n",
      "('these', 1)\n",
      "('they', 2)\n",
      "('this', 5)\n",
      "('those', 1)\n",
      "('threaten', 1)\n",
      "('throughout', 1)\n",
      "('tides', 1)\n",
      "('time', 1)\n",
      "('to', 8)\n",
      "('today', 1)\n",
      "('too', 3)\n",
      "('transition', 1)\n",
      "('true', 1)\n",
      "('trust', 1)\n",
      "('understood', 1)\n",
      "('unity', 1)\n",
      "('us', 1)\n",
      "('use', 1)\n",
      "('violence', 1)\n",
      "('vision', 1)\n",
      "('war', 1)\n",
      "('waters', 1)\n",
      "('ways', 1)\n",
      "('we', 6)\n",
      "('weakened', 1)\n",
      "('well', 2)\n",
      "('will', 2)\n",
      "('with', 1)\n",
      "('words', 1)\n",
      "('worn', 1)\n",
      "('you', 2)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "token_obama_text = tokenizer.tokenize(obama_text)\n",
    "\n",
    "t, num = np.unique(token_obama_text,return_counts=True)\n",
    "\n",
    "for i in range(len(t)):\n",
    "    print((t[i],num[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Term Frequencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'America': 3,\n",
      " 'Americans': 2,\n",
      " 'At': 1,\n",
      " 'Bush': 1,\n",
      " 'But': 1,\n",
      " 'Forty': 1,\n",
      " 'Homes': 1,\n",
      " 'I': 3,\n",
      " 'Less': 1,\n",
      " 'My': 1,\n",
      " 'On': 2,\n",
      " 'Our': 3,\n",
      " 'People': 1,\n",
      " 'President': 1,\n",
      " 'So': 2,\n",
      " 'That': 1,\n",
      " 'The': 1,\n",
      " 'These': 1,\n",
      " 'They': 2,\n",
      " 'Today': 1,\n",
      " 'We': 1,\n",
      " 'Yet': 1,\n",
      " 'a': 6,\n",
      " 'across': 1,\n",
      " 'adversaries': 1,\n",
      " 'against': 1,\n",
      " 'age': 1,\n",
      " 'also': 1,\n",
      " 'amidst': 1,\n",
      " 'an': 1,\n",
      " 'ancestors': 1,\n",
      " 'and': 15,\n",
      " 'are': 5,\n",
      " 'as': 2,\n",
      " 'at': 1,\n",
      " 'badly': 1,\n",
      " 'be': 3,\n",
      " 'because': 3,\n",
      " 'been': 3,\n",
      " 'before': 1,\n",
      " 'bestowed': 1,\n",
      " 'borne': 1,\n",
      " 'brings': 1,\n",
      " 'businesses': 1,\n",
      " 'but': 3,\n",
      " 'by': 2,\n",
      " 'care': 1,\n",
      " 'carried': 1,\n",
      " 'challenges': 1,\n",
      " 'choices': 1,\n",
      " 'chosen': 1,\n",
      " 'citizens': 1,\n",
      " 'clouds': 1,\n",
      " 'collective': 1,\n",
      " 'come': 1,\n",
      " 'confidence': 1,\n",
      " 'conflict': 1,\n",
      " 'consequence': 1,\n",
      " 'cooperation': 1,\n",
      " 'costly': 1,\n",
      " 'crisis': 2,\n",
      " 'data': 1,\n",
      " 'day': 3,\n",
      " 'decline': 1,\n",
      " 'discord': 1,\n",
      " 'documents': 1,\n",
      " 'dogmas': 1,\n",
      " 'during': 1,\n",
      " 'each': 1,\n",
      " 'easily': 1,\n",
      " 'economy': 1,\n",
      " 'end': 1,\n",
      " 'energy': 1,\n",
      " 'every': 1,\n",
      " 'evidence': 1,\n",
      " 'face': 1,\n",
      " 'fail': 1,\n",
      " 'failure': 1,\n",
      " 'faithful': 1,\n",
      " 'false': 1,\n",
      " 'far': 2,\n",
      " 'fear': 2,\n",
      " 'fellow': 1,\n",
      " 'for': 4,\n",
      " 'forbearers': 1,\n",
      " 'founding': 1,\n",
      " 'four': 1,\n",
      " 'further': 1,\n",
      " 'gather': 1,\n",
      " 'gathering': 1,\n",
      " 'generation': 2,\n",
      " 'generosity': 1,\n",
      " 'grateful': 1,\n",
      " 'greed': 1,\n",
      " 'grievances': 1,\n",
      " 'hard': 1,\n",
      " 'has': 3,\n",
      " 'hatred': 1,\n",
      " 'have': 7,\n",
      " 'he': 1,\n",
      " 'health': 1,\n",
      " 'here': 1,\n",
      " 'high': 1,\n",
      " 'his': 1,\n",
      " 'hope': 1,\n",
      " 'humbled': 1,\n",
      " 'ideals': 1,\n",
      " 'in': 3,\n",
      " 'indicators': 1,\n",
      " 'inevitable': 1,\n",
      " 'irresponsibility': 1,\n",
      " 'is': 7,\n",
      " 'it': 2,\n",
      " 'its': 1,\n",
      " 'jobs': 1,\n",
      " 'know': 1,\n",
      " 'land': 1,\n",
      " 'less': 1,\n",
      " 'long': 1,\n",
      " 'lost': 1,\n",
      " 'lower': 1,\n",
      " 'make': 1,\n",
      " 'many': 2,\n",
      " 'measurable': 1,\n",
      " 'met': 2,\n",
      " 'midst': 1,\n",
      " 'mindful': 1,\n",
      " 'moments': 1,\n",
      " 'must': 2,\n",
      " 'nagging': 1,\n",
      " 'nation': 3,\n",
      " 'network': 1,\n",
      " 'new': 1,\n",
      " 'next': 1,\n",
      " 'no': 1,\n",
      " 'not': 2,\n",
      " 'now': 2,\n",
      " 'oath': 2,\n",
      " 'of': 15,\n",
      " 'office': 1,\n",
      " 'often': 1,\n",
      " 'on': 2,\n",
      " 'or': 2,\n",
      " 'our': 10,\n",
      " 'out': 1,\n",
      " 'over': 2,\n",
      " 'part': 1,\n",
      " 'peace': 1,\n",
      " 'petty': 1,\n",
      " 'planet': 1,\n",
      " 'politics': 1,\n",
      " 'prepare': 1,\n",
      " 'presidential': 1,\n",
      " 'proclaim': 1,\n",
      " 'profound': 1,\n",
      " 'promises': 1,\n",
      " 'prosperity': 1,\n",
      " 'purpose': 1,\n",
      " 'raging': 1,\n",
      " 'reaching': 1,\n",
      " 'real': 1,\n",
      " 'recriminations': 1,\n",
      " 'remained': 1,\n",
      " 'rising': 1,\n",
      " 's': 1,\n",
      " 'sacrifices': 1,\n",
      " 'sapping': 1,\n",
      " 'say': 1,\n",
      " 'schools': 1,\n",
      " 'serious': 1,\n",
      " 'service': 1,\n",
      " 'shed': 1,\n",
      " 'short': 1,\n",
      " 'shown': 1,\n",
      " 'shuttered': 1,\n",
      " 'sights': 1,\n",
      " 'simply': 1,\n",
      " 'skill': 1,\n",
      " 'so': 1,\n",
      " 'some': 1,\n",
      " 'span': 1,\n",
      " 'spoken': 1,\n",
      " 'stand': 1,\n",
      " 'statistics': 1,\n",
      " 'still': 1,\n",
      " 'storms': 1,\n",
      " 'strangled': 1,\n",
      " 'strengthen': 1,\n",
      " 'subject': 1,\n",
      " 'taken': 2,\n",
      " 'task': 1,\n",
      " 'thank': 1,\n",
      " 'that': 5,\n",
      " 'the': 19,\n",
      " 'these': 1,\n",
      " 'they': 2,\n",
      " 'this': 5,\n",
      " 'those': 1,\n",
      " 'threaten': 1,\n",
      " 'throughout': 1,\n",
      " 'tides': 1,\n",
      " 'time': 1,\n",
      " 'to': 8,\n",
      " 'today': 1,\n",
      " 'too': 3,\n",
      " 'transition': 1,\n",
      " 'true': 1,\n",
      " 'trust': 1,\n",
      " 'understood': 1,\n",
      " 'unity': 1,\n",
      " 'us': 1,\n",
      " 'use': 1,\n",
      " 'violence': 1,\n",
      " 'vision': 1,\n",
      " 'war': 1,\n",
      " 'waters': 1,\n",
      " 'ways': 1,\n",
      " 'we': 6,\n",
      " 'weakened': 1,\n",
      " 'well': 2,\n",
      " 'will': 2,\n",
      " 'with': 1,\n",
      " 'words': 1,\n",
      " 'worn': 1,\n",
      " 'you': 2}\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "from pprint import pprint\n",
    "freqDist = FreqDist(token_obama_text)\n",
    "words = list(freqDist.keys())\n",
    "\n",
    "pprint(freqDist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Term Frequency-Inverse Document Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  - bigrams e trigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from collections import Counter\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = word_tokenize(obama_text)\n",
    "tokens = [t for t in tokens if t not in stop_words]\n",
    "word_l = WordNetLemmatizer()\n",
    "tokens = [word_l.lemmatize(t) for t in tokens if t.isalpha()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('My', 'fellow'): 1,\n",
       "         ('fellow', 'citizen'): 1,\n",
       "         ('citizen', 'I'): 1,\n",
       "         ('I', 'stand'): 1,\n",
       "         ('stand', 'today'): 1,\n",
       "         ('today', 'humbled'): 1,\n",
       "         ('humbled', 'task'): 1,\n",
       "         ('task', 'u'): 1,\n",
       "         ('u', 'grateful'): 1,\n",
       "         ('grateful', 'trust'): 1,\n",
       "         ('trust', 'bestowed'): 1,\n",
       "         ('bestowed', 'mindful'): 1,\n",
       "         ('mindful', 'sacrifice'): 1,\n",
       "         ('sacrifice', 'borne'): 1,\n",
       "         ('borne', 'ancestor'): 1,\n",
       "         ('ancestor', 'I'): 1,\n",
       "         ('I', 'thank'): 1,\n",
       "         ('thank', 'President'): 1,\n",
       "         ('President', 'Bush'): 1,\n",
       "         ('Bush', 'service'): 1,\n",
       "         ('service', 'nation'): 1,\n",
       "         ('nation', 'well'): 1,\n",
       "         ('well', 'generosity'): 1,\n",
       "         ('generosity', 'cooperation'): 1,\n",
       "         ('cooperation', 'shown'): 1,\n",
       "         ('shown', 'throughout'): 1,\n",
       "         ('throughout', 'transition'): 1,\n",
       "         ('transition', 'Americans'): 1,\n",
       "         ('Americans', 'taken'): 1,\n",
       "         ('taken', 'presidential'): 1,\n",
       "         ('presidential', 'oath'): 1,\n",
       "         ('oath', 'The'): 1,\n",
       "         ('The', 'word'): 1,\n",
       "         ('word', 'spoken'): 1,\n",
       "         ('spoken', 'rising'): 1,\n",
       "         ('rising', 'tide'): 1,\n",
       "         ('tide', 'prosperity'): 1,\n",
       "         ('prosperity', 'still'): 1,\n",
       "         ('still', 'water'): 1,\n",
       "         ('water', 'peace'): 1,\n",
       "         ('peace', 'Yet'): 1,\n",
       "         ('Yet', 'every'): 1,\n",
       "         ('every', 'often'): 1,\n",
       "         ('often', 'oath'): 1,\n",
       "         ('oath', 'taken'): 1,\n",
       "         ('taken', 'amidst'): 1,\n",
       "         ('amidst', 'gathering'): 1,\n",
       "         ('gathering', 'cloud'): 1,\n",
       "         ('cloud', 'raging'): 1,\n",
       "         ('raging', 'storm'): 1,\n",
       "         ('storm', 'At'): 1,\n",
       "         ('At', 'moment'): 1,\n",
       "         ('moment', 'America'): 1,\n",
       "         ('America', 'carried'): 1,\n",
       "         ('carried', 'simply'): 1,\n",
       "         ('simply', 'skill'): 1,\n",
       "         ('skill', 'vision'): 1,\n",
       "         ('vision', 'high'): 1,\n",
       "         ('high', 'office'): 1,\n",
       "         ('office', 'We'): 1,\n",
       "         ('We', 'People'): 1,\n",
       "         ('People', 'remained'): 1,\n",
       "         ('remained', 'faithful'): 1,\n",
       "         ('faithful', 'ideal'): 1,\n",
       "         ('ideal', 'forbearers'): 1,\n",
       "         ('forbearers', 'true'): 1,\n",
       "         ('true', 'founding'): 1,\n",
       "         ('founding', 'document'): 1,\n",
       "         ('document', 'So'): 1,\n",
       "         ('So', 'So'): 1,\n",
       "         ('So', 'must'): 1,\n",
       "         ('must', 'generation'): 1,\n",
       "         ('generation', 'Americans'): 1,\n",
       "         ('Americans', 'That'): 1,\n",
       "         ('That', 'midst'): 1,\n",
       "         ('midst', 'crisis'): 1,\n",
       "         ('crisis', 'well'): 1,\n",
       "         ('well', 'understood'): 1,\n",
       "         ('understood', 'Our'): 1,\n",
       "         ('Our', 'nation'): 1,\n",
       "         ('nation', 'war'): 1,\n",
       "         ('war', 'network'): 1,\n",
       "         ('network', 'violence'): 1,\n",
       "         ('violence', 'hatred'): 1,\n",
       "         ('hatred', 'Our'): 1,\n",
       "         ('Our', 'economy'): 1,\n",
       "         ('economy', 'badly'): 1,\n",
       "         ('badly', 'weakened'): 1,\n",
       "         ('weakened', 'consequence'): 1,\n",
       "         ('consequence', 'greed'): 1,\n",
       "         ('greed', 'irresponsibility'): 1,\n",
       "         ('irresponsibility', 'part'): 1,\n",
       "         ('part', 'also'): 1,\n",
       "         ('also', 'collective'): 1,\n",
       "         ('collective', 'failure'): 1,\n",
       "         ('failure', 'make'): 1,\n",
       "         ('make', 'hard'): 1,\n",
       "         ('hard', 'choice'): 1,\n",
       "         ('choice', 'prepare'): 1,\n",
       "         ('prepare', 'nation'): 1,\n",
       "         ('nation', 'new'): 1,\n",
       "         ('new', 'age'): 1,\n",
       "         ('age', 'Homes'): 1,\n",
       "         ('Homes', 'lost'): 1,\n",
       "         ('lost', 'job'): 1,\n",
       "         ('job', 'shed'): 1,\n",
       "         ('shed', 'business'): 1,\n",
       "         ('business', 'shuttered'): 1,\n",
       "         ('shuttered', 'Our'): 1,\n",
       "         ('Our', 'health'): 1,\n",
       "         ('health', 'care'): 1,\n",
       "         ('care', 'costly'): 1,\n",
       "         ('costly', 'school'): 1,\n",
       "         ('school', 'fail'): 1,\n",
       "         ('fail', 'many'): 1,\n",
       "         ('many', 'day'): 1,\n",
       "         ('day', 'brings'): 1,\n",
       "         ('brings', 'evidence'): 1,\n",
       "         ('evidence', 'way'): 1,\n",
       "         ('way', 'use'): 1,\n",
       "         ('use', 'energy'): 1,\n",
       "         ('energy', 'strengthen'): 1,\n",
       "         ('strengthen', 'adversary'): 1,\n",
       "         ('adversary', 'threaten'): 1,\n",
       "         ('threaten', 'planet'): 1,\n",
       "         ('planet', 'These'): 1,\n",
       "         ('These', 'indicator'): 1,\n",
       "         ('indicator', 'crisis'): 1,\n",
       "         ('crisis', 'subject'): 1,\n",
       "         ('subject', 'data'): 1,\n",
       "         ('data', 'statistic'): 1,\n",
       "         ('statistic', 'Less'): 1,\n",
       "         ('Less', 'measurable'): 1,\n",
       "         ('measurable', 'le'): 1,\n",
       "         ('le', 'profound'): 1,\n",
       "         ('profound', 'sapping'): 1,\n",
       "         ('sapping', 'confidence'): 1,\n",
       "         ('confidence', 'across'): 1,\n",
       "         ('across', 'land'): 1,\n",
       "         ('land', 'nagging'): 1,\n",
       "         ('nagging', 'fear'): 1,\n",
       "         ('fear', 'America'): 1,\n",
       "         ('America', 'decline'): 1,\n",
       "         ('decline', 'inevitable'): 1,\n",
       "         ('inevitable', 'next'): 1,\n",
       "         ('next', 'generation'): 1,\n",
       "         ('generation', 'must'): 1,\n",
       "         ('must', 'lower'): 1,\n",
       "         ('lower', 'sight'): 1,\n",
       "         ('sight', 'Today'): 1,\n",
       "         ('Today', 'I'): 1,\n",
       "         ('I', 'say'): 1,\n",
       "         ('say', 'challenge'): 1,\n",
       "         ('challenge', 'face'): 1,\n",
       "         ('face', 'real'): 1,\n",
       "         ('real', 'They'): 1,\n",
       "         ('They', 'serious'): 1,\n",
       "         ('serious', 'many'): 1,\n",
       "         ('many', 'They'): 1,\n",
       "         ('They', 'met'): 1,\n",
       "         ('met', 'easily'): 1,\n",
       "         ('easily', 'short'): 1,\n",
       "         ('short', 'span'): 1,\n",
       "         ('span', 'time'): 1,\n",
       "         ('time', 'But'): 1,\n",
       "         ('But', 'know'): 1,\n",
       "         ('know', 'America'): 1,\n",
       "         ('America', 'met'): 1,\n",
       "         ('met', 'On'): 1,\n",
       "         ('On', 'day'): 2,\n",
       "         ('day', 'gather'): 1,\n",
       "         ('gather', 'chosen'): 1,\n",
       "         ('chosen', 'hope'): 1,\n",
       "         ('hope', 'fear'): 1,\n",
       "         ('fear', 'unity'): 1,\n",
       "         ('unity', 'purpose'): 1,\n",
       "         ('purpose', 'conflict'): 1,\n",
       "         ('conflict', 'discord'): 1,\n",
       "         ('discord', 'On'): 1,\n",
       "         ('day', 'come'): 1,\n",
       "         ('come', 'proclaim'): 1,\n",
       "         ('proclaim', 'end'): 1,\n",
       "         ('end', 'petty'): 1,\n",
       "         ('petty', 'grievance'): 1,\n",
       "         ('grievance', 'false'): 1,\n",
       "         ('false', 'promise'): 1,\n",
       "         ('promise', 'recrimination'): 1,\n",
       "         ('recrimination', 'worn'): 1,\n",
       "         ('worn', 'dogma'): 1,\n",
       "         ('dogma', 'far'): 1,\n",
       "         ('far', 'long'): 1,\n",
       "         ('long', 'strangled'): 1,\n",
       "         ('strangled', 'politics'): 1})"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BIGRAMS\n",
    "Counter(list(ngrams(tokens,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('My', 'fellow', 'citizen'): 1,\n",
       "         ('fellow', 'citizen', 'I'): 1,\n",
       "         ('citizen', 'I', 'stand'): 1,\n",
       "         ('I', 'stand', 'today'): 1,\n",
       "         ('stand', 'today', 'humbled'): 1,\n",
       "         ('today', 'humbled', 'task'): 1,\n",
       "         ('humbled', 'task', 'u'): 1,\n",
       "         ('task', 'u', 'grateful'): 1,\n",
       "         ('u', 'grateful', 'trust'): 1,\n",
       "         ('grateful', 'trust', 'bestowed'): 1,\n",
       "         ('trust', 'bestowed', 'mindful'): 1,\n",
       "         ('bestowed', 'mindful', 'sacrifice'): 1,\n",
       "         ('mindful', 'sacrifice', 'borne'): 1,\n",
       "         ('sacrifice', 'borne', 'ancestor'): 1,\n",
       "         ('borne', 'ancestor', 'I'): 1,\n",
       "         ('ancestor', 'I', 'thank'): 1,\n",
       "         ('I', 'thank', 'President'): 1,\n",
       "         ('thank', 'President', 'Bush'): 1,\n",
       "         ('President', 'Bush', 'service'): 1,\n",
       "         ('Bush', 'service', 'nation'): 1,\n",
       "         ('service', 'nation', 'well'): 1,\n",
       "         ('nation', 'well', 'generosity'): 1,\n",
       "         ('well', 'generosity', 'cooperation'): 1,\n",
       "         ('generosity', 'cooperation', 'shown'): 1,\n",
       "         ('cooperation', 'shown', 'throughout'): 1,\n",
       "         ('shown', 'throughout', 'transition'): 1,\n",
       "         ('throughout', 'transition', 'Americans'): 1,\n",
       "         ('transition', 'Americans', 'taken'): 1,\n",
       "         ('Americans', 'taken', 'presidential'): 1,\n",
       "         ('taken', 'presidential', 'oath'): 1,\n",
       "         ('presidential', 'oath', 'The'): 1,\n",
       "         ('oath', 'The', 'word'): 1,\n",
       "         ('The', 'word', 'spoken'): 1,\n",
       "         ('word', 'spoken', 'rising'): 1,\n",
       "         ('spoken', 'rising', 'tide'): 1,\n",
       "         ('rising', 'tide', 'prosperity'): 1,\n",
       "         ('tide', 'prosperity', 'still'): 1,\n",
       "         ('prosperity', 'still', 'water'): 1,\n",
       "         ('still', 'water', 'peace'): 1,\n",
       "         ('water', 'peace', 'Yet'): 1,\n",
       "         ('peace', 'Yet', 'every'): 1,\n",
       "         ('Yet', 'every', 'often'): 1,\n",
       "         ('every', 'often', 'oath'): 1,\n",
       "         ('often', 'oath', 'taken'): 1,\n",
       "         ('oath', 'taken', 'amidst'): 1,\n",
       "         ('taken', 'amidst', 'gathering'): 1,\n",
       "         ('amidst', 'gathering', 'cloud'): 1,\n",
       "         ('gathering', 'cloud', 'raging'): 1,\n",
       "         ('cloud', 'raging', 'storm'): 1,\n",
       "         ('raging', 'storm', 'At'): 1,\n",
       "         ('storm', 'At', 'moment'): 1,\n",
       "         ('At', 'moment', 'America'): 1,\n",
       "         ('moment', 'America', 'carried'): 1,\n",
       "         ('America', 'carried', 'simply'): 1,\n",
       "         ('carried', 'simply', 'skill'): 1,\n",
       "         ('simply', 'skill', 'vision'): 1,\n",
       "         ('skill', 'vision', 'high'): 1,\n",
       "         ('vision', 'high', 'office'): 1,\n",
       "         ('high', 'office', 'We'): 1,\n",
       "         ('office', 'We', 'People'): 1,\n",
       "         ('We', 'People', 'remained'): 1,\n",
       "         ('People', 'remained', 'faithful'): 1,\n",
       "         ('remained', 'faithful', 'ideal'): 1,\n",
       "         ('faithful', 'ideal', 'forbearers'): 1,\n",
       "         ('ideal', 'forbearers', 'true'): 1,\n",
       "         ('forbearers', 'true', 'founding'): 1,\n",
       "         ('true', 'founding', 'document'): 1,\n",
       "         ('founding', 'document', 'So'): 1,\n",
       "         ('document', 'So', 'So'): 1,\n",
       "         ('So', 'So', 'must'): 1,\n",
       "         ('So', 'must', 'generation'): 1,\n",
       "         ('must', 'generation', 'Americans'): 1,\n",
       "         ('generation', 'Americans', 'That'): 1,\n",
       "         ('Americans', 'That', 'midst'): 1,\n",
       "         ('That', 'midst', 'crisis'): 1,\n",
       "         ('midst', 'crisis', 'well'): 1,\n",
       "         ('crisis', 'well', 'understood'): 1,\n",
       "         ('well', 'understood', 'Our'): 1,\n",
       "         ('understood', 'Our', 'nation'): 1,\n",
       "         ('Our', 'nation', 'war'): 1,\n",
       "         ('nation', 'war', 'network'): 1,\n",
       "         ('war', 'network', 'violence'): 1,\n",
       "         ('network', 'violence', 'hatred'): 1,\n",
       "         ('violence', 'hatred', 'Our'): 1,\n",
       "         ('hatred', 'Our', 'economy'): 1,\n",
       "         ('Our', 'economy', 'badly'): 1,\n",
       "         ('economy', 'badly', 'weakened'): 1,\n",
       "         ('badly', 'weakened', 'consequence'): 1,\n",
       "         ('weakened', 'consequence', 'greed'): 1,\n",
       "         ('consequence', 'greed', 'irresponsibility'): 1,\n",
       "         ('greed', 'irresponsibility', 'part'): 1,\n",
       "         ('irresponsibility', 'part', 'also'): 1,\n",
       "         ('part', 'also', 'collective'): 1,\n",
       "         ('also', 'collective', 'failure'): 1,\n",
       "         ('collective', 'failure', 'make'): 1,\n",
       "         ('failure', 'make', 'hard'): 1,\n",
       "         ('make', 'hard', 'choice'): 1,\n",
       "         ('hard', 'choice', 'prepare'): 1,\n",
       "         ('choice', 'prepare', 'nation'): 1,\n",
       "         ('prepare', 'nation', 'new'): 1,\n",
       "         ('nation', 'new', 'age'): 1,\n",
       "         ('new', 'age', 'Homes'): 1,\n",
       "         ('age', 'Homes', 'lost'): 1,\n",
       "         ('Homes', 'lost', 'job'): 1,\n",
       "         ('lost', 'job', 'shed'): 1,\n",
       "         ('job', 'shed', 'business'): 1,\n",
       "         ('shed', 'business', 'shuttered'): 1,\n",
       "         ('business', 'shuttered', 'Our'): 1,\n",
       "         ('shuttered', 'Our', 'health'): 1,\n",
       "         ('Our', 'health', 'care'): 1,\n",
       "         ('health', 'care', 'costly'): 1,\n",
       "         ('care', 'costly', 'school'): 1,\n",
       "         ('costly', 'school', 'fail'): 1,\n",
       "         ('school', 'fail', 'many'): 1,\n",
       "         ('fail', 'many', 'day'): 1,\n",
       "         ('many', 'day', 'brings'): 1,\n",
       "         ('day', 'brings', 'evidence'): 1,\n",
       "         ('brings', 'evidence', 'way'): 1,\n",
       "         ('evidence', 'way', 'use'): 1,\n",
       "         ('way', 'use', 'energy'): 1,\n",
       "         ('use', 'energy', 'strengthen'): 1,\n",
       "         ('energy', 'strengthen', 'adversary'): 1,\n",
       "         ('strengthen', 'adversary', 'threaten'): 1,\n",
       "         ('adversary', 'threaten', 'planet'): 1,\n",
       "         ('threaten', 'planet', 'These'): 1,\n",
       "         ('planet', 'These', 'indicator'): 1,\n",
       "         ('These', 'indicator', 'crisis'): 1,\n",
       "         ('indicator', 'crisis', 'subject'): 1,\n",
       "         ('crisis', 'subject', 'data'): 1,\n",
       "         ('subject', 'data', 'statistic'): 1,\n",
       "         ('data', 'statistic', 'Less'): 1,\n",
       "         ('statistic', 'Less', 'measurable'): 1,\n",
       "         ('Less', 'measurable', 'le'): 1,\n",
       "         ('measurable', 'le', 'profound'): 1,\n",
       "         ('le', 'profound', 'sapping'): 1,\n",
       "         ('profound', 'sapping', 'confidence'): 1,\n",
       "         ('sapping', 'confidence', 'across'): 1,\n",
       "         ('confidence', 'across', 'land'): 1,\n",
       "         ('across', 'land', 'nagging'): 1,\n",
       "         ('land', 'nagging', 'fear'): 1,\n",
       "         ('nagging', 'fear', 'America'): 1,\n",
       "         ('fear', 'America', 'decline'): 1,\n",
       "         ('America', 'decline', 'inevitable'): 1,\n",
       "         ('decline', 'inevitable', 'next'): 1,\n",
       "         ('inevitable', 'next', 'generation'): 1,\n",
       "         ('next', 'generation', 'must'): 1,\n",
       "         ('generation', 'must', 'lower'): 1,\n",
       "         ('must', 'lower', 'sight'): 1,\n",
       "         ('lower', 'sight', 'Today'): 1,\n",
       "         ('sight', 'Today', 'I'): 1,\n",
       "         ('Today', 'I', 'say'): 1,\n",
       "         ('I', 'say', 'challenge'): 1,\n",
       "         ('say', 'challenge', 'face'): 1,\n",
       "         ('challenge', 'face', 'real'): 1,\n",
       "         ('face', 'real', 'They'): 1,\n",
       "         ('real', 'They', 'serious'): 1,\n",
       "         ('They', 'serious', 'many'): 1,\n",
       "         ('serious', 'many', 'They'): 1,\n",
       "         ('many', 'They', 'met'): 1,\n",
       "         ('They', 'met', 'easily'): 1,\n",
       "         ('met', 'easily', 'short'): 1,\n",
       "         ('easily', 'short', 'span'): 1,\n",
       "         ('short', 'span', 'time'): 1,\n",
       "         ('span', 'time', 'But'): 1,\n",
       "         ('time', 'But', 'know'): 1,\n",
       "         ('But', 'know', 'America'): 1,\n",
       "         ('know', 'America', 'met'): 1,\n",
       "         ('America', 'met', 'On'): 1,\n",
       "         ('met', 'On', 'day'): 1,\n",
       "         ('On', 'day', 'gather'): 1,\n",
       "         ('day', 'gather', 'chosen'): 1,\n",
       "         ('gather', 'chosen', 'hope'): 1,\n",
       "         ('chosen', 'hope', 'fear'): 1,\n",
       "         ('hope', 'fear', 'unity'): 1,\n",
       "         ('fear', 'unity', 'purpose'): 1,\n",
       "         ('unity', 'purpose', 'conflict'): 1,\n",
       "         ('purpose', 'conflict', 'discord'): 1,\n",
       "         ('conflict', 'discord', 'On'): 1,\n",
       "         ('discord', 'On', 'day'): 1,\n",
       "         ('On', 'day', 'come'): 1,\n",
       "         ('day', 'come', 'proclaim'): 1,\n",
       "         ('come', 'proclaim', 'end'): 1,\n",
       "         ('proclaim', 'end', 'petty'): 1,\n",
       "         ('end', 'petty', 'grievance'): 1,\n",
       "         ('petty', 'grievance', 'false'): 1,\n",
       "         ('grievance', 'false', 'promise'): 1,\n",
       "         ('false', 'promise', 'recrimination'): 1,\n",
       "         ('promise', 'recrimination', 'worn'): 1,\n",
       "         ('recrimination', 'worn', 'dogma'): 1,\n",
       "         ('worn', 'dogma', 'far'): 1,\n",
       "         ('dogma', 'far', 'long'): 1,\n",
       "         ('far', 'long', 'strangled'): 1,\n",
       "         ('long', 'strangled', 'politics'): 1})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRIGAMS\n",
    "Counter(list(ngrams(tokens,3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
