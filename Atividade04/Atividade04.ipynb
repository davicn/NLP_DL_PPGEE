{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atividade 04 -  Text Preprocessing\n",
    "\n",
    "## Davi Costa Nascimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1. Tokenization \n",
    "    - a) Generates a list of tokenized words\n",
    "- 2. Cleaning \n",
    "    - a) Lower casing \n",
    "    - b) Removal of Punctuations \n",
    "    - c) Removal of Stopwords \n",
    "    - d) Removal of Frequent words \n",
    "    - e) Removal of Rare words \n",
    "    - f) Removal of emojis \n",
    "    - g) Removal of emoticons \n",
    "    - h) Removal of URLs \n",
    "    - i) Removal of HTML tags/markups \n",
    "    - j) Chat words conversion \n",
    "    - k) Spelling correction\n",
    "- 3. Normalization \n",
    "    - a) Converting dates to text \n",
    "    - b) Numbers to text \n",
    "    - c) Currency/Percent signs to text \n",
    "    - d) Spelling mistakes correction \n",
    "    - e) Conversion of emoticons to words \n",
    "    - f) Conversion of emojis to words\n",
    "- 4. Lemmatization\n",
    "- 5. Steming\n",
    "- 6. Consider a CORPUS of texts and convert text data to vectors of numbers \n",
    "    - a) A vocabulary of known words (tokens) is extracted from the text, the occurence of words is scored, and the resulting numerical values are saved in vocabulary-long vectors. There are a few versions of BoW, corresponding to different words scoring methods.\n",
    "        - Binary (wheter the word is in the text or not)\n",
    "        - Word Counts ( number of word occurrences in the text)\n",
    "        - Term Frequencies\n",
    "        - Term Frequency-Inverse Document Frequencies\n",
    "        - bigrams e trigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿a capivara (nome científico: hydrochoerus hydrochaeris) é uma espécie de\n",
      "mamífero roedor da família caviidae e subfamília hydrochoerinae. alguns autores\n",
      "consideram que deva ser classificada em uma família própria. está incluída no\n",
      "mesmo grupo de roedores ao qual se classificam as pacas, cutias, os preás e o\n",
      "porquinho-da-índia. ocorre por toda a américa do sul ao leste dos andes em\n",
      "habitats associados a rios, lagos e pântanos, do nível do mar até 1 300 m de\n",
      "altitude. extremamente adaptável, pode ocorrer em ambientes altamente alterados\n",
      "pelo ser humano.\n",
      "\n",
      "\n",
      "é o maior roedor do mundo, pesando até 91 kg e medindo até 1,2 m de comprimento\n",
      "e 60 cm de altura. a pelagem é densa, de cor avermelhada a marrom escuro. é\n",
      "possível distinguir os machos por conta da presença de uma glândula proeminente\n",
      "no focinho apesar do dimorfismo sexual não ser aparente. existe uma série de\n",
      "adaptações no sistema digestório à herbivoria, principalmente no ceco. alcança\n",
      "a maturidade sexual com cerca de 1,5 ano de idade, e as fêmeas dão à luz\n",
      "geralmente a quatro filhotes por vez, pesando até 1,5 kg e já nascem com pelos\n",
      "e dentição permanente. em cativeiro, pode viver até 12 anos de idade.\n"
     ]
    }
   ],
   "source": [
    "# Carregando texto\n",
    "\n",
    "with open('files/capivara-pt.txt','r') as f:\n",
    "    text = f.readlines()\n",
    "    \n",
    "text = ''.join(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Generates a list of tokenized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'capivara',\n",
       " 'nome',\n",
       " 'científico',\n",
       " 'hydrochoerus',\n",
       " 'hydrochaeris',\n",
       " 'é',\n",
       " 'uma',\n",
       " 'espécie',\n",
       " 'de',\n",
       " 'mamífero',\n",
       " 'roedor',\n",
       " 'da',\n",
       " 'família',\n",
       " 'caviidae',\n",
       " 'e',\n",
       " 'subfamília',\n",
       " 'hydrochoerinae',\n",
       " 'alguns',\n",
       " 'autores',\n",
       " 'consideram',\n",
       " 'que',\n",
       " 'deva',\n",
       " 'ser',\n",
       " 'classificada',\n",
       " 'em',\n",
       " 'uma',\n",
       " 'família',\n",
       " 'própria',\n",
       " 'está',\n",
       " 'incluída',\n",
       " 'no',\n",
       " 'mesmo',\n",
       " 'grupo',\n",
       " 'de',\n",
       " 'roedores',\n",
       " 'ao',\n",
       " 'qual',\n",
       " 'se',\n",
       " 'classificam',\n",
       " 'as',\n",
       " 'pacas',\n",
       " 'cutias',\n",
       " 'os',\n",
       " 'preás',\n",
       " 'e',\n",
       " 'o',\n",
       " 'porquinho',\n",
       " 'da',\n",
       " 'índia',\n",
       " 'ocorre',\n",
       " 'por',\n",
       " 'toda',\n",
       " 'a',\n",
       " 'américa',\n",
       " 'do',\n",
       " 'sul',\n",
       " 'ao',\n",
       " 'leste',\n",
       " 'dos',\n",
       " 'andes',\n",
       " 'em',\n",
       " 'habitats',\n",
       " 'associados',\n",
       " 'a',\n",
       " 'rios',\n",
       " 'lagos',\n",
       " 'e',\n",
       " 'pântanos',\n",
       " 'do',\n",
       " 'nível',\n",
       " 'do',\n",
       " 'mar',\n",
       " 'até',\n",
       " '1',\n",
       " '300',\n",
       " 'm',\n",
       " 'de',\n",
       " 'altitude',\n",
       " 'extremamente',\n",
       " 'adaptável',\n",
       " 'pode',\n",
       " 'ocorrer',\n",
       " 'em',\n",
       " 'ambientes',\n",
       " 'altamente',\n",
       " 'alterados',\n",
       " 'pelo',\n",
       " 'ser',\n",
       " 'humano',\n",
       " 'é',\n",
       " 'o',\n",
       " 'maior',\n",
       " 'roedor',\n",
       " 'do',\n",
       " 'mundo',\n",
       " 'pesando',\n",
       " 'até',\n",
       " '91',\n",
       " 'kg',\n",
       " 'e',\n",
       " 'medindo',\n",
       " 'até',\n",
       " '1',\n",
       " '2',\n",
       " 'm',\n",
       " 'de',\n",
       " 'comprimento',\n",
       " 'e',\n",
       " '60',\n",
       " 'cm',\n",
       " 'de',\n",
       " 'altura',\n",
       " 'a',\n",
       " 'pelagem',\n",
       " 'é',\n",
       " 'densa',\n",
       " 'de',\n",
       " 'cor',\n",
       " 'avermelhada',\n",
       " 'a',\n",
       " 'marrom',\n",
       " 'escuro',\n",
       " 'é',\n",
       " 'possível',\n",
       " 'distinguir',\n",
       " 'os',\n",
       " 'machos',\n",
       " 'por',\n",
       " 'conta',\n",
       " 'da',\n",
       " 'presença',\n",
       " 'de',\n",
       " 'uma',\n",
       " 'glândula',\n",
       " 'proeminente',\n",
       " 'no',\n",
       " 'focinho',\n",
       " 'apesar',\n",
       " 'do',\n",
       " 'dimorfismo',\n",
       " 'sexual',\n",
       " 'não',\n",
       " 'ser',\n",
       " 'aparente',\n",
       " 'existe',\n",
       " 'uma',\n",
       " 'série',\n",
       " 'de',\n",
       " 'adaptações',\n",
       " 'no',\n",
       " 'sistema',\n",
       " 'digestório',\n",
       " 'à',\n",
       " 'herbivoria',\n",
       " 'principalmente',\n",
       " 'no',\n",
       " 'ceco',\n",
       " 'alcança',\n",
       " 'a',\n",
       " 'maturidade',\n",
       " 'sexual',\n",
       " 'com',\n",
       " 'cerca',\n",
       " 'de',\n",
       " '1',\n",
       " '5',\n",
       " 'ano',\n",
       " 'de',\n",
       " 'idade',\n",
       " 'e',\n",
       " 'as',\n",
       " 'fêmeas',\n",
       " 'dão',\n",
       " 'à',\n",
       " 'luz',\n",
       " 'geralmente',\n",
       " 'a',\n",
       " 'quatro',\n",
       " 'filhotes',\n",
       " 'por',\n",
       " 'vez',\n",
       " 'pesando',\n",
       " 'até',\n",
       " '1',\n",
       " '5',\n",
       " 'kg',\n",
       " 'e',\n",
       " 'já',\n",
       " 'nascem',\n",
       " 'com',\n",
       " 'pelos',\n",
       " 'e',\n",
       " 'dentição',\n",
       " 'permanente',\n",
       " 'em',\n",
       " 'cativeiro',\n",
       " 'pode',\n",
       " 'viver',\n",
       " 'até',\n",
       " '12',\n",
       " 'anos',\n",
       " 'de',\n",
       " 'idade']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "# Aplicando Tokenização por palavras\n",
    "token_list = tokenizer.tokenize(text)\n",
    "\n",
    "token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Lower casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'capivara',\n",
       " 'nome',\n",
       " 'científico',\n",
       " 'hydrochoerus',\n",
       " 'hydrochaeris',\n",
       " 'é',\n",
       " 'uma',\n",
       " 'espécie',\n",
       " 'de',\n",
       " 'mamífero',\n",
       " 'roedor',\n",
       " 'da',\n",
       " 'família',\n",
       " 'caviidae',\n",
       " 'e',\n",
       " 'subfamília',\n",
       " 'hydrochoerinae',\n",
       " 'alguns',\n",
       " 'autores',\n",
       " 'consideram',\n",
       " 'que',\n",
       " 'deva',\n",
       " 'ser',\n",
       " 'classificada',\n",
       " 'em',\n",
       " 'uma',\n",
       " 'família',\n",
       " 'própria',\n",
       " 'está',\n",
       " 'incluída',\n",
       " 'no',\n",
       " 'mesmo',\n",
       " 'grupo',\n",
       " 'de',\n",
       " 'roedores',\n",
       " 'ao',\n",
       " 'qual',\n",
       " 'se',\n",
       " 'classificam',\n",
       " 'as',\n",
       " 'pacas',\n",
       " 'cutias',\n",
       " 'os',\n",
       " 'preás',\n",
       " 'e',\n",
       " 'o',\n",
       " 'porquinho',\n",
       " 'da',\n",
       " 'índia',\n",
       " 'ocorre',\n",
       " 'por',\n",
       " 'toda',\n",
       " 'a',\n",
       " 'américa',\n",
       " 'do',\n",
       " 'sul',\n",
       " 'ao',\n",
       " 'leste',\n",
       " 'dos',\n",
       " 'andes',\n",
       " 'em',\n",
       " 'habitats',\n",
       " 'associados',\n",
       " 'a',\n",
       " 'rios',\n",
       " 'lagos',\n",
       " 'e',\n",
       " 'pântanos',\n",
       " 'do',\n",
       " 'nível',\n",
       " 'do',\n",
       " 'mar',\n",
       " 'até',\n",
       " '1',\n",
       " '300',\n",
       " 'm',\n",
       " 'de',\n",
       " 'altitude',\n",
       " 'extremamente',\n",
       " 'adaptável',\n",
       " 'pode',\n",
       " 'ocorrer',\n",
       " 'em',\n",
       " 'ambientes',\n",
       " 'altamente',\n",
       " 'alterados',\n",
       " 'pelo',\n",
       " 'ser',\n",
       " 'humano',\n",
       " 'é',\n",
       " 'o',\n",
       " 'maior',\n",
       " 'roedor',\n",
       " 'do',\n",
       " 'mundo',\n",
       " 'pesando',\n",
       " 'até',\n",
       " '91',\n",
       " 'kg',\n",
       " 'e',\n",
       " 'medindo',\n",
       " 'até',\n",
       " '1',\n",
       " '2',\n",
       " 'm',\n",
       " 'de',\n",
       " 'comprimento',\n",
       " 'e',\n",
       " '60',\n",
       " 'cm',\n",
       " 'de',\n",
       " 'altura',\n",
       " 'a',\n",
       " 'pelagem',\n",
       " 'é',\n",
       " 'densa',\n",
       " 'de',\n",
       " 'cor',\n",
       " 'avermelhada',\n",
       " 'a',\n",
       " 'marrom',\n",
       " 'escuro',\n",
       " 'é',\n",
       " 'possível',\n",
       " 'distinguir',\n",
       " 'os',\n",
       " 'machos',\n",
       " 'por',\n",
       " 'conta',\n",
       " 'da',\n",
       " 'presença',\n",
       " 'de',\n",
       " 'uma',\n",
       " 'glândula',\n",
       " 'proeminente',\n",
       " 'no',\n",
       " 'focinho',\n",
       " 'apesar',\n",
       " 'do',\n",
       " 'dimorfismo',\n",
       " 'sexual',\n",
       " 'não',\n",
       " 'ser',\n",
       " 'aparente',\n",
       " 'existe',\n",
       " 'uma',\n",
       " 'série',\n",
       " 'de',\n",
       " 'adaptações',\n",
       " 'no',\n",
       " 'sistema',\n",
       " 'digestório',\n",
       " 'à',\n",
       " 'herbivoria',\n",
       " 'principalmente',\n",
       " 'no',\n",
       " 'ceco',\n",
       " 'alcança',\n",
       " 'a',\n",
       " 'maturidade',\n",
       " 'sexual',\n",
       " 'com',\n",
       " 'cerca',\n",
       " 'de',\n",
       " '1',\n",
       " '5',\n",
       " 'ano',\n",
       " 'de',\n",
       " 'idade',\n",
       " 'e',\n",
       " 'as',\n",
       " 'fêmeas',\n",
       " 'dão',\n",
       " 'à',\n",
       " 'luz',\n",
       " 'geralmente',\n",
       " 'a',\n",
       " 'quatro',\n",
       " 'filhotes',\n",
       " 'por',\n",
       " 'vez',\n",
       " 'pesando',\n",
       " 'até',\n",
       " '1',\n",
       " '5',\n",
       " 'kg',\n",
       " 'e',\n",
       " 'já',\n",
       " 'nascem',\n",
       " 'com',\n",
       " 'pelos',\n",
       " 'e',\n",
       " 'dentição',\n",
       " 'permanente',\n",
       " 'em',\n",
       " 'cativeiro',\n",
       " 'pode',\n",
       " 'viver',\n",
       " 'até',\n",
       " '12',\n",
       " 'anos',\n",
       " 'de',\n",
       " 'idade']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_token = [i.lower()for i in token_list]\n",
    "lower_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\tRemoval of Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Isso é bom    Isso é muito    '"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str = \"Isso é bom ?: Isso é muito ! ;\"\n",
    "punc = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''\n",
    "\n",
    "for ele in test_str:  \n",
    "    if ele in punc:  \n",
    "        test_str = test_str.replace(ele, \" \")\n",
    "\n",
    "test_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  c)\tRemoval of Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/davi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['capivara',\n",
       " 'nome',\n",
       " 'científico',\n",
       " 'hydrochoerus',\n",
       " 'hydrochaeris',\n",
       " 'espécie',\n",
       " 'mamífero',\n",
       " 'roedor',\n",
       " 'família',\n",
       " 'caviidae',\n",
       " 'subfamília',\n",
       " 'hydrochoerinae',\n",
       " 'alguns',\n",
       " 'autores',\n",
       " 'consideram',\n",
       " 'deva',\n",
       " 'ser',\n",
       " 'classificada',\n",
       " 'família',\n",
       " 'própria',\n",
       " 'incluída',\n",
       " 'grupo',\n",
       " 'roedores',\n",
       " 'classificam',\n",
       " 'pacas',\n",
       " 'cutias',\n",
       " 'preás',\n",
       " 'porquinho',\n",
       " 'índia',\n",
       " 'ocorre',\n",
       " 'toda',\n",
       " 'américa',\n",
       " 'sul',\n",
       " 'leste',\n",
       " 'andes',\n",
       " 'habitats',\n",
       " 'associados',\n",
       " 'rios',\n",
       " 'lagos',\n",
       " 'pântanos',\n",
       " 'nível',\n",
       " 'mar',\n",
       " '1',\n",
       " '300',\n",
       " 'm',\n",
       " 'altitude',\n",
       " 'extremamente',\n",
       " 'adaptável',\n",
       " 'pode',\n",
       " 'ocorrer',\n",
       " 'ambientes',\n",
       " 'altamente',\n",
       " 'alterados',\n",
       " 'ser',\n",
       " 'humano',\n",
       " 'maior',\n",
       " 'roedor',\n",
       " 'mundo',\n",
       " 'pesando',\n",
       " '91',\n",
       " 'kg',\n",
       " 'medindo',\n",
       " '1',\n",
       " '2',\n",
       " 'm',\n",
       " 'comprimento',\n",
       " '60',\n",
       " 'cm',\n",
       " 'altura',\n",
       " 'pelagem',\n",
       " 'densa',\n",
       " 'cor',\n",
       " 'avermelhada',\n",
       " 'marrom',\n",
       " 'escuro',\n",
       " 'possível',\n",
       " 'distinguir',\n",
       " 'machos',\n",
       " 'conta',\n",
       " 'presença',\n",
       " 'glândula',\n",
       " 'proeminente',\n",
       " 'focinho',\n",
       " 'apesar',\n",
       " 'dimorfismo',\n",
       " 'sexual',\n",
       " 'ser',\n",
       " 'aparente',\n",
       " 'existe',\n",
       " 'série',\n",
       " 'adaptações',\n",
       " 'sistema',\n",
       " 'digestório',\n",
       " 'herbivoria',\n",
       " 'principalmente',\n",
       " 'ceco',\n",
       " 'alcança',\n",
       " 'maturidade',\n",
       " 'sexual',\n",
       " 'cerca',\n",
       " '1',\n",
       " '5',\n",
       " 'ano',\n",
       " 'idade',\n",
       " 'fêmeas',\n",
       " 'dão',\n",
       " 'luz',\n",
       " 'geralmente',\n",
       " 'quatro',\n",
       " 'filhotes',\n",
       " 'vez',\n",
       " 'pesando',\n",
       " '1',\n",
       " '5',\n",
       " 'kg',\n",
       " 'nascem',\n",
       " 'dentição',\n",
       " 'permanente',\n",
       " 'cativeiro',\n",
       " 'pode',\n",
       " 'viver',\n",
       " '12',\n",
       " 'anos',\n",
       " 'idade']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download de stopworks em português\n",
    "nltk.download('stopwords')\n",
    "stop_works = set(stopwords.words('portuguese'))\n",
    "\n",
    "filter_words = [i for i in token_list if not i in stop_works]\n",
    "\n",
    "# lista de palavras sem stopwroks\n",
    "filter_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   d)\tRemoval of Frequent words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   e)\tRemoval of Rare words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'nome',\n",
       " 'científico',\n",
       " 'hydrochoerus',\n",
       " 'hydrochaeris',\n",
       " 'é',\n",
       " 'uma',\n",
       " 'espécie',\n",
       " 'de',\n",
       " 'mamífero',\n",
       " 'roedor',\n",
       " 'da',\n",
       " 'família',\n",
       " 'caviidae',\n",
       " 'e',\n",
       " 'subfamília',\n",
       " 'hydrochoerinae',\n",
       " 'alguns',\n",
       " 'autores',\n",
       " 'consideram',\n",
       " 'que',\n",
       " 'deva',\n",
       " 'ser',\n",
       " 'classificada',\n",
       " 'em',\n",
       " 'uma',\n",
       " 'família',\n",
       " 'própria',\n",
       " 'está',\n",
       " 'incluída',\n",
       " 'no',\n",
       " 'mesmo',\n",
       " 'grupo',\n",
       " 'de',\n",
       " 'roedores',\n",
       " 'ao',\n",
       " 'qual',\n",
       " 'se',\n",
       " 'classificam',\n",
       " 'as',\n",
       " 'pacas',\n",
       " 'cutias',\n",
       " 'os',\n",
       " 'preás',\n",
       " 'e',\n",
       " 'o',\n",
       " 'porquinho',\n",
       " 'da',\n",
       " 'índia',\n",
       " 'ocorre',\n",
       " 'por',\n",
       " 'toda',\n",
       " 'a',\n",
       " 'américa',\n",
       " 'do',\n",
       " 'sul',\n",
       " 'ao',\n",
       " 'leste',\n",
       " 'dos',\n",
       " 'andes',\n",
       " 'em',\n",
       " 'habitats',\n",
       " 'associados',\n",
       " 'a',\n",
       " 'rios',\n",
       " 'lagos',\n",
       " 'e',\n",
       " 'pântanos',\n",
       " 'do',\n",
       " 'nível',\n",
       " 'do',\n",
       " 'mar',\n",
       " 'até',\n",
       " '1',\n",
       " '300',\n",
       " 'm',\n",
       " 'de',\n",
       " 'altitude',\n",
       " 'extremamente',\n",
       " 'adaptável',\n",
       " 'pode',\n",
       " 'ocorrer',\n",
       " 'em',\n",
       " 'ambientes',\n",
       " 'altamente',\n",
       " 'alterados',\n",
       " 'pelo',\n",
       " 'ser',\n",
       " 'humano',\n",
       " 'é',\n",
       " 'o',\n",
       " 'maior',\n",
       " 'roedor',\n",
       " 'do',\n",
       " 'mundo',\n",
       " 'pesando',\n",
       " 'até',\n",
       " '91',\n",
       " 'kg',\n",
       " 'e',\n",
       " 'medindo',\n",
       " 'até',\n",
       " '1',\n",
       " '2',\n",
       " 'm',\n",
       " 'de',\n",
       " 'comprimento',\n",
       " 'e',\n",
       " '60',\n",
       " 'cm',\n",
       " 'de',\n",
       " 'altura',\n",
       " 'a',\n",
       " 'pelagem',\n",
       " 'é',\n",
       " 'densa',\n",
       " 'de',\n",
       " 'cor',\n",
       " 'avermelhada',\n",
       " 'a',\n",
       " 'marrom',\n",
       " 'escuro',\n",
       " 'é',\n",
       " 'possível',\n",
       " 'distinguir',\n",
       " 'os',\n",
       " 'machos',\n",
       " 'por',\n",
       " 'conta',\n",
       " 'da',\n",
       " 'presença',\n",
       " 'de',\n",
       " 'uma',\n",
       " 'glândula',\n",
       " 'proeminente',\n",
       " 'no',\n",
       " 'focinho',\n",
       " 'apesar',\n",
       " 'do',\n",
       " 'dimorfismo',\n",
       " 'sexual',\n",
       " 'não',\n",
       " 'ser',\n",
       " 'aparente',\n",
       " 'existe',\n",
       " 'uma',\n",
       " 'série',\n",
       " 'de',\n",
       " 'adaptações',\n",
       " 'no',\n",
       " 'sistema',\n",
       " 'digestório',\n",
       " 'à',\n",
       " 'herbivoria',\n",
       " 'principalmente',\n",
       " 'no',\n",
       " 'ceco',\n",
       " 'alcança',\n",
       " 'a',\n",
       " 'maturidade',\n",
       " 'sexual',\n",
       " 'com',\n",
       " 'cerca',\n",
       " 'de',\n",
       " '1',\n",
       " '5',\n",
       " 'ano',\n",
       " 'de',\n",
       " 'idade',\n",
       " 'e',\n",
       " 'as',\n",
       " 'fêmeas',\n",
       " 'dão',\n",
       " 'à',\n",
       " 'luz',\n",
       " 'geralmente',\n",
       " 'a',\n",
       " 'quatro',\n",
       " 'filhotes',\n",
       " 'por',\n",
       " 'vez',\n",
       " 'pesando',\n",
       " 'até',\n",
       " '1',\n",
       " '5',\n",
       " 'kg',\n",
       " 'e',\n",
       " 'já',\n",
       " 'nascem',\n",
       " 'com',\n",
       " 'pelos',\n",
       " 'e',\n",
       " 'dentição',\n",
       " 'permanente',\n",
       " 'em',\n",
       " 'cativeiro',\n",
       " 'pode',\n",
       " 'viver',\n",
       " 'até',\n",
       " '12',\n",
       " 'anos',\n",
       " 'de',\n",
       " 'idade']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list.remove('capivara')\n",
    "token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   f)\tRemoval of emojis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Um  😂 texto 😎 com 😒 emojis\n",
      "Um   texto  com  emojis\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def emoji(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", \n",
    "    flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "text = \"Um  😂 texto 😎 com 😒 emojis\"\n",
    "\n",
    "# Texto com emojis\n",
    "print(text)\n",
    "# Texto sem emojis\n",
    "print(emoji(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   g)\tRemoval of emoticons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   h)\tRemoval of URLs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Os dados estão disponíveis em: https://physionet.org/content/adfecgdb/1.0.0/\n",
      "Os dados estão disponíveis em: \n"
     ]
    }
   ],
   "source": [
    "url = 'https://physionet.org/content/adfecgdb/1.0.0/'\n",
    "texto = 'Os dados estão disponíveis em: ' + url\n",
    "\n",
    "# texto com url\n",
    "print(texto)\n",
    "\n",
    "new_texto = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "# texto sem url\n",
    "print(new_texto.sub(r\"\",texto))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   i)\tRemoval of HTML tags/markups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "\n",
      "    <head>\n",
      "        <meta charset=\"UTF-8\">\n",
      "        <title>Meu título</title>\n",
      "    </head>\n",
      "\n",
      "    <body>\n",
      "        <h1>Meu texto H1</h1>\n",
      "        <p>Meu parágrafo</p>\n",
      "    </body>\n",
      "\n",
      "</html>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "        \n",
      "        Meu título\n",
      "    \n",
      "\n",
      "    \n",
      "        Meu texto H1\n",
      "        Meu parágrafo\n",
      "    \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "html_text = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Meu título</title>\n",
    "    </head>\n",
    "\n",
    "    <body>\n",
    "        <h1>Meu texto H1</h1>\n",
    "        <p>Meu parágrafo</p>\n",
    "    </body>\n",
    "\n",
    "</html>\"\"\"\n",
    "\n",
    "# texto com HTML\n",
    "print(html_text)\n",
    "\n",
    "new_html_text = re.compile(r\"<.*?>\")\n",
    "\n",
    "# texto sem HTML\n",
    "print(new_html_text.sub(r\"\",html_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   j)\tChat words conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   k)\tSpelling correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\tNormalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  a)\tConverting dates to text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-18\n",
      "2020-October-18\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "mydate = datetime.datetime.today()\n",
    "\n",
    "print(mydate.strftime('%Y-%m-%d'))\n",
    "\n",
    "print(mydate.strftime(\"%Y\")+\"-\"+mydate.strftime(\"%B\")+\"-\"+mydate.strftime(\"%d\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   b)\tNumbers to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ten'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install inflect\n",
    "\n",
    "import inflect\n",
    "\n",
    "N = 10\n",
    "\n",
    "ie = inflect.engine()\n",
    "num_text = ie.number_to_words(10)\n",
    "num_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   c)\tCurrency/Percent signs to text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O preço era $30,55, mas houve 25% de desconto\n",
      "O preço era reais 30,55, mas houve 25 por cento de desconto\n"
     ]
    }
   ],
   "source": [
    "money_text = 'O preço era $30,55, mas houve 25% de desconto'\n",
    "\n",
    "\n",
    "# texto com \n",
    "print(money_text)\n",
    "\n",
    "# texto sem \n",
    "print(money_text.replace('$','reais ').replace('%',' por cento'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   d)\tSpelling mistakes correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   e)\tConversion of emoticons to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   f)\tConversion of emojis to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estudar NLP é :thumbs_up:\n"
     ]
    }
   ],
   "source": [
    "# !pip install emoji\n",
    "\n",
    "import emoji\n",
    "\n",
    "print(emoji.demojize('Estudar NLP é 👍'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\tLemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I  -  I\n",
      "'m  -  'm\n",
      "always  -  always\n",
      "learning  -  learning\n",
      "and  -  and\n",
      "looking  -  looking\n",
      "for  -  for\n",
      "news  -  news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/davi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "# word_data = \"É preciso se comportar bem\"\n",
    "word_data = \"I'm always learning and looking for news\"\n",
    "\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "for w in nltk_tokens:\n",
    "    print(w,\" - \",wl.lemmatize(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\tSteming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I  -  I\n",
      "'m  -  'm\n",
      "always  -  alway\n",
      "learning  -  learn\n",
      "and  -  and\n",
      "looking  -  look\n",
      "for  -  for\n",
      "news  -  news\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "word_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "word_data = \"I'm always learning and looking for news\"\n",
    "\n",
    "# First Word tokenization\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "#Next find the roots of the word\n",
    "for w in nltk_tokens:\n",
    "       print(w,\" - \",porter_stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Consider a CORPUS of texts and convert text data to vectors of numbers\n",
    "\n",
    "### a) A vocabulary of known words (tokens) is extracted from the text, the occurence of words is scored, and the resulting numerical values are saved in vocabulary-long vectors. There are a few versions of BoW, corresponding to different words scoring methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "My fellow citizens:\n",
      "\n",
      "I stand here today humbled by the task before us, grateful for the trust you have bestowed, mindful of the sacrifices borne by our ancestors. I thank President Bush for his service to our nation, as well as the generosity and cooperation he has shown throughout this transition. \n",
      "Forty-four Americans have now taken the presidential oath. The words have been spoken during rising tides of prosperity and the still waters of peace. Yet, every so often the oath is taken amidst gathering clouds and raging storms. At these moments, America has carried on not simply because of the skill or vision of those in high office, but because We the People have remained faithful to the ideals of our forbearers, and true to our founding documents.\n",
      "\n",
      "So it has been. So it must be with this generation of Americans.\n",
      "\n",
      "That we are in the midst of crisis is now well understood. Our nation is at war, against a far-reaching network of violence and hatred. Our economy is badly weakened, a consequence of greed and irresponsibility on the part of some, but also our collective failure to make hard choices and prepare the nation for a new age. Homes have been lost; jobs shed; businesses shuttered. Our health care is too costly; our schools fail too many; and each day brings further evidence that the ways we use energy strengthen our adversaries and threaten our planet.\n",
      "These are the indicators of crisis, subject to data and statistics. Less measurable but no less profound is a sapping of confidence across our land - a nagging fear that America's decline is inevitable, and that the next generation must lower its sights.\n",
      "Today I say to you that the challenges we face are real. They are serious and they are many. They will not be met easily or in a short span of time. But know this, America - they will be met.\n",
      "On this day, we gather because we have chosen hope over fear, unity of purpose over conflict and discord.\n",
      "On this day, we come to proclaim an end to the petty grievances and false promises, the recriminations and worn out dogmas, that for far too long have strangled our politics.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "obama_text = \"\"\"\n",
    "My fellow citizens:\n",
    "\n",
    "I stand here today humbled by the task before us, grateful for the trust you have bestowed, mindful of the sacrifices borne by our ancestors. I thank President Bush for his service to our nation, as well as the generosity and cooperation he has shown throughout this transition. \n",
    "Forty-four Americans have now taken the presidential oath. The words have been spoken during rising tides of prosperity and the still waters of peace. Yet, every so often the oath is taken amidst gathering clouds and raging storms. At these moments, America has carried on not simply because of the skill or vision of those in high office, but because We the People have remained faithful to the ideals of our forbearers, and true to our founding documents.\n",
    "\n",
    "So it has been. So it must be with this generation of Americans.\n",
    "\n",
    "That we are in the midst of crisis is now well understood. Our nation is at war, against a far-reaching network of violence and hatred. Our economy is badly weakened, a consequence of greed and irresponsibility on the part of some, but also our collective failure to make hard choices and prepare the nation for a new age. Homes have been lost; jobs shed; businesses shuttered. Our health care is too costly; our schools fail too many; and each day brings further evidence that the ways we use energy strengthen our adversaries and threaten our planet.\n",
    "These are the indicators of crisis, subject to data and statistics. Less measurable but no less profound is a sapping of confidence across our land - a nagging fear that America's decline is inevitable, and that the next generation must lower its sights.\n",
    "Today I say to you that the challenges we face are real. They are serious and they are many. They will not be met easily or in a short span of time. But know this, America - they will be met.\n",
    "On this day, we gather because we have chosen hope over fear, unity of purpose over conflict and discord.\n",
    "On this day, we come to proclaim an end to the petty grievances and false promises, the recriminations and worn out dogmas, that for far too long have strangled our politics.\n",
    "\"\"\"\n",
    "print(obama_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Binary (wheter the word is in the text or not)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Word Counts ( number of word occurrences in the text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('America', 3)\n",
      "('Americans', 2)\n",
      "('At', 1)\n",
      "('Bush', 1)\n",
      "('But', 1)\n",
      "('Forty', 1)\n",
      "('Homes', 1)\n",
      "('I', 3)\n",
      "('Less', 1)\n",
      "('My', 1)\n",
      "('On', 2)\n",
      "('Our', 3)\n",
      "('People', 1)\n",
      "('President', 1)\n",
      "('So', 2)\n",
      "('That', 1)\n",
      "('The', 1)\n",
      "('These', 1)\n",
      "('They', 2)\n",
      "('Today', 1)\n",
      "('We', 1)\n",
      "('Yet', 1)\n",
      "('a', 6)\n",
      "('across', 1)\n",
      "('adversaries', 1)\n",
      "('against', 1)\n",
      "('age', 1)\n",
      "('also', 1)\n",
      "('amidst', 1)\n",
      "('an', 1)\n",
      "('ancestors', 1)\n",
      "('and', 15)\n",
      "('are', 5)\n",
      "('as', 2)\n",
      "('at', 1)\n",
      "('badly', 1)\n",
      "('be', 3)\n",
      "('because', 3)\n",
      "('been', 3)\n",
      "('before', 1)\n",
      "('bestowed', 1)\n",
      "('borne', 1)\n",
      "('brings', 1)\n",
      "('businesses', 1)\n",
      "('but', 3)\n",
      "('by', 2)\n",
      "('care', 1)\n",
      "('carried', 1)\n",
      "('challenges', 1)\n",
      "('choices', 1)\n",
      "('chosen', 1)\n",
      "('citizens', 1)\n",
      "('clouds', 1)\n",
      "('collective', 1)\n",
      "('come', 1)\n",
      "('confidence', 1)\n",
      "('conflict', 1)\n",
      "('consequence', 1)\n",
      "('cooperation', 1)\n",
      "('costly', 1)\n",
      "('crisis', 2)\n",
      "('data', 1)\n",
      "('day', 3)\n",
      "('decline', 1)\n",
      "('discord', 1)\n",
      "('documents', 1)\n",
      "('dogmas', 1)\n",
      "('during', 1)\n",
      "('each', 1)\n",
      "('easily', 1)\n",
      "('economy', 1)\n",
      "('end', 1)\n",
      "('energy', 1)\n",
      "('every', 1)\n",
      "('evidence', 1)\n",
      "('face', 1)\n",
      "('fail', 1)\n",
      "('failure', 1)\n",
      "('faithful', 1)\n",
      "('false', 1)\n",
      "('far', 2)\n",
      "('fear', 2)\n",
      "('fellow', 1)\n",
      "('for', 4)\n",
      "('forbearers', 1)\n",
      "('founding', 1)\n",
      "('four', 1)\n",
      "('further', 1)\n",
      "('gather', 1)\n",
      "('gathering', 1)\n",
      "('generation', 2)\n",
      "('generosity', 1)\n",
      "('grateful', 1)\n",
      "('greed', 1)\n",
      "('grievances', 1)\n",
      "('hard', 1)\n",
      "('has', 3)\n",
      "('hatred', 1)\n",
      "('have', 7)\n",
      "('he', 1)\n",
      "('health', 1)\n",
      "('here', 1)\n",
      "('high', 1)\n",
      "('his', 1)\n",
      "('hope', 1)\n",
      "('humbled', 1)\n",
      "('ideals', 1)\n",
      "('in', 3)\n",
      "('indicators', 1)\n",
      "('inevitable', 1)\n",
      "('irresponsibility', 1)\n",
      "('is', 7)\n",
      "('it', 2)\n",
      "('its', 1)\n",
      "('jobs', 1)\n",
      "('know', 1)\n",
      "('land', 1)\n",
      "('less', 1)\n",
      "('long', 1)\n",
      "('lost', 1)\n",
      "('lower', 1)\n",
      "('make', 1)\n",
      "('many', 2)\n",
      "('measurable', 1)\n",
      "('met', 2)\n",
      "('midst', 1)\n",
      "('mindful', 1)\n",
      "('moments', 1)\n",
      "('must', 2)\n",
      "('nagging', 1)\n",
      "('nation', 3)\n",
      "('network', 1)\n",
      "('new', 1)\n",
      "('next', 1)\n",
      "('no', 1)\n",
      "('not', 2)\n",
      "('now', 2)\n",
      "('oath', 2)\n",
      "('of', 15)\n",
      "('office', 1)\n",
      "('often', 1)\n",
      "('on', 2)\n",
      "('or', 2)\n",
      "('our', 10)\n",
      "('out', 1)\n",
      "('over', 2)\n",
      "('part', 1)\n",
      "('peace', 1)\n",
      "('petty', 1)\n",
      "('planet', 1)\n",
      "('politics', 1)\n",
      "('prepare', 1)\n",
      "('presidential', 1)\n",
      "('proclaim', 1)\n",
      "('profound', 1)\n",
      "('promises', 1)\n",
      "('prosperity', 1)\n",
      "('purpose', 1)\n",
      "('raging', 1)\n",
      "('reaching', 1)\n",
      "('real', 1)\n",
      "('recriminations', 1)\n",
      "('remained', 1)\n",
      "('rising', 1)\n",
      "('s', 1)\n",
      "('sacrifices', 1)\n",
      "('sapping', 1)\n",
      "('say', 1)\n",
      "('schools', 1)\n",
      "('serious', 1)\n",
      "('service', 1)\n",
      "('shed', 1)\n",
      "('short', 1)\n",
      "('shown', 1)\n",
      "('shuttered', 1)\n",
      "('sights', 1)\n",
      "('simply', 1)\n",
      "('skill', 1)\n",
      "('so', 1)\n",
      "('some', 1)\n",
      "('span', 1)\n",
      "('spoken', 1)\n",
      "('stand', 1)\n",
      "('statistics', 1)\n",
      "('still', 1)\n",
      "('storms', 1)\n",
      "('strangled', 1)\n",
      "('strengthen', 1)\n",
      "('subject', 1)\n",
      "('taken', 2)\n",
      "('task', 1)\n",
      "('thank', 1)\n",
      "('that', 5)\n",
      "('the', 19)\n",
      "('these', 1)\n",
      "('they', 2)\n",
      "('this', 5)\n",
      "('those', 1)\n",
      "('threaten', 1)\n",
      "('throughout', 1)\n",
      "('tides', 1)\n",
      "('time', 1)\n",
      "('to', 8)\n",
      "('today', 1)\n",
      "('too', 3)\n",
      "('transition', 1)\n",
      "('true', 1)\n",
      "('trust', 1)\n",
      "('understood', 1)\n",
      "('unity', 1)\n",
      "('us', 1)\n",
      "('use', 1)\n",
      "('violence', 1)\n",
      "('vision', 1)\n",
      "('war', 1)\n",
      "('waters', 1)\n",
      "('ways', 1)\n",
      "('we', 6)\n",
      "('weakened', 1)\n",
      "('well', 2)\n",
      "('will', 2)\n",
      "('with', 1)\n",
      "('words', 1)\n",
      "('worn', 1)\n",
      "('you', 2)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "token_obama_text = tokenizer.tokenize(obama_text)\n",
    "\n",
    "t, num = np.unique(token_obama_text,return_counts=True)\n",
    "\n",
    "for i in range(len(t)):\n",
    "    print((t[i],num[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Term Frequencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'America': 3,\n",
      " 'Americans': 2,\n",
      " 'At': 1,\n",
      " 'Bush': 1,\n",
      " 'But': 1,\n",
      " 'Forty': 1,\n",
      " 'Homes': 1,\n",
      " 'I': 3,\n",
      " 'Less': 1,\n",
      " 'My': 1,\n",
      " 'On': 2,\n",
      " 'Our': 3,\n",
      " 'People': 1,\n",
      " 'President': 1,\n",
      " 'So': 2,\n",
      " 'That': 1,\n",
      " 'The': 1,\n",
      " 'These': 1,\n",
      " 'They': 2,\n",
      " 'Today': 1,\n",
      " 'We': 1,\n",
      " 'Yet': 1,\n",
      " 'a': 6,\n",
      " 'across': 1,\n",
      " 'adversaries': 1,\n",
      " 'against': 1,\n",
      " 'age': 1,\n",
      " 'also': 1,\n",
      " 'amidst': 1,\n",
      " 'an': 1,\n",
      " 'ancestors': 1,\n",
      " 'and': 15,\n",
      " 'are': 5,\n",
      " 'as': 2,\n",
      " 'at': 1,\n",
      " 'badly': 1,\n",
      " 'be': 3,\n",
      " 'because': 3,\n",
      " 'been': 3,\n",
      " 'before': 1,\n",
      " 'bestowed': 1,\n",
      " 'borne': 1,\n",
      " 'brings': 1,\n",
      " 'businesses': 1,\n",
      " 'but': 3,\n",
      " 'by': 2,\n",
      " 'care': 1,\n",
      " 'carried': 1,\n",
      " 'challenges': 1,\n",
      " 'choices': 1,\n",
      " 'chosen': 1,\n",
      " 'citizens': 1,\n",
      " 'clouds': 1,\n",
      " 'collective': 1,\n",
      " 'come': 1,\n",
      " 'confidence': 1,\n",
      " 'conflict': 1,\n",
      " 'consequence': 1,\n",
      " 'cooperation': 1,\n",
      " 'costly': 1,\n",
      " 'crisis': 2,\n",
      " 'data': 1,\n",
      " 'day': 3,\n",
      " 'decline': 1,\n",
      " 'discord': 1,\n",
      " 'documents': 1,\n",
      " 'dogmas': 1,\n",
      " 'during': 1,\n",
      " 'each': 1,\n",
      " 'easily': 1,\n",
      " 'economy': 1,\n",
      " 'end': 1,\n",
      " 'energy': 1,\n",
      " 'every': 1,\n",
      " 'evidence': 1,\n",
      " 'face': 1,\n",
      " 'fail': 1,\n",
      " 'failure': 1,\n",
      " 'faithful': 1,\n",
      " 'false': 1,\n",
      " 'far': 2,\n",
      " 'fear': 2,\n",
      " 'fellow': 1,\n",
      " 'for': 4,\n",
      " 'forbearers': 1,\n",
      " 'founding': 1,\n",
      " 'four': 1,\n",
      " 'further': 1,\n",
      " 'gather': 1,\n",
      " 'gathering': 1,\n",
      " 'generation': 2,\n",
      " 'generosity': 1,\n",
      " 'grateful': 1,\n",
      " 'greed': 1,\n",
      " 'grievances': 1,\n",
      " 'hard': 1,\n",
      " 'has': 3,\n",
      " 'hatred': 1,\n",
      " 'have': 7,\n",
      " 'he': 1,\n",
      " 'health': 1,\n",
      " 'here': 1,\n",
      " 'high': 1,\n",
      " 'his': 1,\n",
      " 'hope': 1,\n",
      " 'humbled': 1,\n",
      " 'ideals': 1,\n",
      " 'in': 3,\n",
      " 'indicators': 1,\n",
      " 'inevitable': 1,\n",
      " 'irresponsibility': 1,\n",
      " 'is': 7,\n",
      " 'it': 2,\n",
      " 'its': 1,\n",
      " 'jobs': 1,\n",
      " 'know': 1,\n",
      " 'land': 1,\n",
      " 'less': 1,\n",
      " 'long': 1,\n",
      " 'lost': 1,\n",
      " 'lower': 1,\n",
      " 'make': 1,\n",
      " 'many': 2,\n",
      " 'measurable': 1,\n",
      " 'met': 2,\n",
      " 'midst': 1,\n",
      " 'mindful': 1,\n",
      " 'moments': 1,\n",
      " 'must': 2,\n",
      " 'nagging': 1,\n",
      " 'nation': 3,\n",
      " 'network': 1,\n",
      " 'new': 1,\n",
      " 'next': 1,\n",
      " 'no': 1,\n",
      " 'not': 2,\n",
      " 'now': 2,\n",
      " 'oath': 2,\n",
      " 'of': 15,\n",
      " 'office': 1,\n",
      " 'often': 1,\n",
      " 'on': 2,\n",
      " 'or': 2,\n",
      " 'our': 10,\n",
      " 'out': 1,\n",
      " 'over': 2,\n",
      " 'part': 1,\n",
      " 'peace': 1,\n",
      " 'petty': 1,\n",
      " 'planet': 1,\n",
      " 'politics': 1,\n",
      " 'prepare': 1,\n",
      " 'presidential': 1,\n",
      " 'proclaim': 1,\n",
      " 'profound': 1,\n",
      " 'promises': 1,\n",
      " 'prosperity': 1,\n",
      " 'purpose': 1,\n",
      " 'raging': 1,\n",
      " 'reaching': 1,\n",
      " 'real': 1,\n",
      " 'recriminations': 1,\n",
      " 'remained': 1,\n",
      " 'rising': 1,\n",
      " 's': 1,\n",
      " 'sacrifices': 1,\n",
      " 'sapping': 1,\n",
      " 'say': 1,\n",
      " 'schools': 1,\n",
      " 'serious': 1,\n",
      " 'service': 1,\n",
      " 'shed': 1,\n",
      " 'short': 1,\n",
      " 'shown': 1,\n",
      " 'shuttered': 1,\n",
      " 'sights': 1,\n",
      " 'simply': 1,\n",
      " 'skill': 1,\n",
      " 'so': 1,\n",
      " 'some': 1,\n",
      " 'span': 1,\n",
      " 'spoken': 1,\n",
      " 'stand': 1,\n",
      " 'statistics': 1,\n",
      " 'still': 1,\n",
      " 'storms': 1,\n",
      " 'strangled': 1,\n",
      " 'strengthen': 1,\n",
      " 'subject': 1,\n",
      " 'taken': 2,\n",
      " 'task': 1,\n",
      " 'thank': 1,\n",
      " 'that': 5,\n",
      " 'the': 19,\n",
      " 'these': 1,\n",
      " 'they': 2,\n",
      " 'this': 5,\n",
      " 'those': 1,\n",
      " 'threaten': 1,\n",
      " 'throughout': 1,\n",
      " 'tides': 1,\n",
      " 'time': 1,\n",
      " 'to': 8,\n",
      " 'today': 1,\n",
      " 'too': 3,\n",
      " 'transition': 1,\n",
      " 'true': 1,\n",
      " 'trust': 1,\n",
      " 'understood': 1,\n",
      " 'unity': 1,\n",
      " 'us': 1,\n",
      " 'use': 1,\n",
      " 'violence': 1,\n",
      " 'vision': 1,\n",
      " 'war': 1,\n",
      " 'waters': 1,\n",
      " 'ways': 1,\n",
      " 'we': 6,\n",
      " 'weakened': 1,\n",
      " 'well': 2,\n",
      " 'will': 2,\n",
      " 'with': 1,\n",
      " 'words': 1,\n",
      " 'worn': 1,\n",
      " 'you': 2}\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "from pprint import pprint\n",
    "freqDist = FreqDist(token_obama_text)\n",
    "words = list(freqDist.keys())\n",
    "\n",
    "pprint(freqDist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Term Frequency-Inverse Document Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  - bigrams e trigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from collections import Counter\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = word_tokenize(obama_text)\n",
    "tokens = [t for t in tokens if t not in stop_words]\n",
    "word_l = WordNetLemmatizer()\n",
    "tokens = [word_l.lemmatize(t) for t in tokens if t.isalpha()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('My', 'fellow'): 1,\n",
       "         ('fellow', 'citizen'): 1,\n",
       "         ('citizen', 'I'): 1,\n",
       "         ('I', 'stand'): 1,\n",
       "         ('stand', 'today'): 1,\n",
       "         ('today', 'humbled'): 1,\n",
       "         ('humbled', 'task'): 1,\n",
       "         ('task', 'u'): 1,\n",
       "         ('u', 'grateful'): 1,\n",
       "         ('grateful', 'trust'): 1,\n",
       "         ('trust', 'bestowed'): 1,\n",
       "         ('bestowed', 'mindful'): 1,\n",
       "         ('mindful', 'sacrifice'): 1,\n",
       "         ('sacrifice', 'borne'): 1,\n",
       "         ('borne', 'ancestor'): 1,\n",
       "         ('ancestor', 'I'): 1,\n",
       "         ('I', 'thank'): 1,\n",
       "         ('thank', 'President'): 1,\n",
       "         ('President', 'Bush'): 1,\n",
       "         ('Bush', 'service'): 1,\n",
       "         ('service', 'nation'): 1,\n",
       "         ('nation', 'well'): 1,\n",
       "         ('well', 'generosity'): 1,\n",
       "         ('generosity', 'cooperation'): 1,\n",
       "         ('cooperation', 'shown'): 1,\n",
       "         ('shown', 'throughout'): 1,\n",
       "         ('throughout', 'transition'): 1,\n",
       "         ('transition', 'Americans'): 1,\n",
       "         ('Americans', 'taken'): 1,\n",
       "         ('taken', 'presidential'): 1,\n",
       "         ('presidential', 'oath'): 1,\n",
       "         ('oath', 'The'): 1,\n",
       "         ('The', 'word'): 1,\n",
       "         ('word', 'spoken'): 1,\n",
       "         ('spoken', 'rising'): 1,\n",
       "         ('rising', 'tide'): 1,\n",
       "         ('tide', 'prosperity'): 1,\n",
       "         ('prosperity', 'still'): 1,\n",
       "         ('still', 'water'): 1,\n",
       "         ('water', 'peace'): 1,\n",
       "         ('peace', 'Yet'): 1,\n",
       "         ('Yet', 'every'): 1,\n",
       "         ('every', 'often'): 1,\n",
       "         ('often', 'oath'): 1,\n",
       "         ('oath', 'taken'): 1,\n",
       "         ('taken', 'amidst'): 1,\n",
       "         ('amidst', 'gathering'): 1,\n",
       "         ('gathering', 'cloud'): 1,\n",
       "         ('cloud', 'raging'): 1,\n",
       "         ('raging', 'storm'): 1,\n",
       "         ('storm', 'At'): 1,\n",
       "         ('At', 'moment'): 1,\n",
       "         ('moment', 'America'): 1,\n",
       "         ('America', 'carried'): 1,\n",
       "         ('carried', 'simply'): 1,\n",
       "         ('simply', 'skill'): 1,\n",
       "         ('skill', 'vision'): 1,\n",
       "         ('vision', 'high'): 1,\n",
       "         ('high', 'office'): 1,\n",
       "         ('office', 'We'): 1,\n",
       "         ('We', 'People'): 1,\n",
       "         ('People', 'remained'): 1,\n",
       "         ('remained', 'faithful'): 1,\n",
       "         ('faithful', 'ideal'): 1,\n",
       "         ('ideal', 'forbearers'): 1,\n",
       "         ('forbearers', 'true'): 1,\n",
       "         ('true', 'founding'): 1,\n",
       "         ('founding', 'document'): 1,\n",
       "         ('document', 'So'): 1,\n",
       "         ('So', 'So'): 1,\n",
       "         ('So', 'must'): 1,\n",
       "         ('must', 'generation'): 1,\n",
       "         ('generation', 'Americans'): 1,\n",
       "         ('Americans', 'That'): 1,\n",
       "         ('That', 'midst'): 1,\n",
       "         ('midst', 'crisis'): 1,\n",
       "         ('crisis', 'well'): 1,\n",
       "         ('well', 'understood'): 1,\n",
       "         ('understood', 'Our'): 1,\n",
       "         ('Our', 'nation'): 1,\n",
       "         ('nation', 'war'): 1,\n",
       "         ('war', 'network'): 1,\n",
       "         ('network', 'violence'): 1,\n",
       "         ('violence', 'hatred'): 1,\n",
       "         ('hatred', 'Our'): 1,\n",
       "         ('Our', 'economy'): 1,\n",
       "         ('economy', 'badly'): 1,\n",
       "         ('badly', 'weakened'): 1,\n",
       "         ('weakened', 'consequence'): 1,\n",
       "         ('consequence', 'greed'): 1,\n",
       "         ('greed', 'irresponsibility'): 1,\n",
       "         ('irresponsibility', 'part'): 1,\n",
       "         ('part', 'also'): 1,\n",
       "         ('also', 'collective'): 1,\n",
       "         ('collective', 'failure'): 1,\n",
       "         ('failure', 'make'): 1,\n",
       "         ('make', 'hard'): 1,\n",
       "         ('hard', 'choice'): 1,\n",
       "         ('choice', 'prepare'): 1,\n",
       "         ('prepare', 'nation'): 1,\n",
       "         ('nation', 'new'): 1,\n",
       "         ('new', 'age'): 1,\n",
       "         ('age', 'Homes'): 1,\n",
       "         ('Homes', 'lost'): 1,\n",
       "         ('lost', 'job'): 1,\n",
       "         ('job', 'shed'): 1,\n",
       "         ('shed', 'business'): 1,\n",
       "         ('business', 'shuttered'): 1,\n",
       "         ('shuttered', 'Our'): 1,\n",
       "         ('Our', 'health'): 1,\n",
       "         ('health', 'care'): 1,\n",
       "         ('care', 'costly'): 1,\n",
       "         ('costly', 'school'): 1,\n",
       "         ('school', 'fail'): 1,\n",
       "         ('fail', 'many'): 1,\n",
       "         ('many', 'day'): 1,\n",
       "         ('day', 'brings'): 1,\n",
       "         ('brings', 'evidence'): 1,\n",
       "         ('evidence', 'way'): 1,\n",
       "         ('way', 'use'): 1,\n",
       "         ('use', 'energy'): 1,\n",
       "         ('energy', 'strengthen'): 1,\n",
       "         ('strengthen', 'adversary'): 1,\n",
       "         ('adversary', 'threaten'): 1,\n",
       "         ('threaten', 'planet'): 1,\n",
       "         ('planet', 'These'): 1,\n",
       "         ('These', 'indicator'): 1,\n",
       "         ('indicator', 'crisis'): 1,\n",
       "         ('crisis', 'subject'): 1,\n",
       "         ('subject', 'data'): 1,\n",
       "         ('data', 'statistic'): 1,\n",
       "         ('statistic', 'Less'): 1,\n",
       "         ('Less', 'measurable'): 1,\n",
       "         ('measurable', 'le'): 1,\n",
       "         ('le', 'profound'): 1,\n",
       "         ('profound', 'sapping'): 1,\n",
       "         ('sapping', 'confidence'): 1,\n",
       "         ('confidence', 'across'): 1,\n",
       "         ('across', 'land'): 1,\n",
       "         ('land', 'nagging'): 1,\n",
       "         ('nagging', 'fear'): 1,\n",
       "         ('fear', 'America'): 1,\n",
       "         ('America', 'decline'): 1,\n",
       "         ('decline', 'inevitable'): 1,\n",
       "         ('inevitable', 'next'): 1,\n",
       "         ('next', 'generation'): 1,\n",
       "         ('generation', 'must'): 1,\n",
       "         ('must', 'lower'): 1,\n",
       "         ('lower', 'sight'): 1,\n",
       "         ('sight', 'Today'): 1,\n",
       "         ('Today', 'I'): 1,\n",
       "         ('I', 'say'): 1,\n",
       "         ('say', 'challenge'): 1,\n",
       "         ('challenge', 'face'): 1,\n",
       "         ('face', 'real'): 1,\n",
       "         ('real', 'They'): 1,\n",
       "         ('They', 'serious'): 1,\n",
       "         ('serious', 'many'): 1,\n",
       "         ('many', 'They'): 1,\n",
       "         ('They', 'met'): 1,\n",
       "         ('met', 'easily'): 1,\n",
       "         ('easily', 'short'): 1,\n",
       "         ('short', 'span'): 1,\n",
       "         ('span', 'time'): 1,\n",
       "         ('time', 'But'): 1,\n",
       "         ('But', 'know'): 1,\n",
       "         ('know', 'America'): 1,\n",
       "         ('America', 'met'): 1,\n",
       "         ('met', 'On'): 1,\n",
       "         ('On', 'day'): 2,\n",
       "         ('day', 'gather'): 1,\n",
       "         ('gather', 'chosen'): 1,\n",
       "         ('chosen', 'hope'): 1,\n",
       "         ('hope', 'fear'): 1,\n",
       "         ('fear', 'unity'): 1,\n",
       "         ('unity', 'purpose'): 1,\n",
       "         ('purpose', 'conflict'): 1,\n",
       "         ('conflict', 'discord'): 1,\n",
       "         ('discord', 'On'): 1,\n",
       "         ('day', 'come'): 1,\n",
       "         ('come', 'proclaim'): 1,\n",
       "         ('proclaim', 'end'): 1,\n",
       "         ('end', 'petty'): 1,\n",
       "         ('petty', 'grievance'): 1,\n",
       "         ('grievance', 'false'): 1,\n",
       "         ('false', 'promise'): 1,\n",
       "         ('promise', 'recrimination'): 1,\n",
       "         ('recrimination', 'worn'): 1,\n",
       "         ('worn', 'dogma'): 1,\n",
       "         ('dogma', 'far'): 1,\n",
       "         ('far', 'long'): 1,\n",
       "         ('long', 'strangled'): 1,\n",
       "         ('strangled', 'politics'): 1})"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BIGRAMS\n",
    "Counter(list(ngrams(tokens,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('My', 'fellow', 'citizen'): 1,\n",
       "         ('fellow', 'citizen', 'I'): 1,\n",
       "         ('citizen', 'I', 'stand'): 1,\n",
       "         ('I', 'stand', 'today'): 1,\n",
       "         ('stand', 'today', 'humbled'): 1,\n",
       "         ('today', 'humbled', 'task'): 1,\n",
       "         ('humbled', 'task', 'u'): 1,\n",
       "         ('task', 'u', 'grateful'): 1,\n",
       "         ('u', 'grateful', 'trust'): 1,\n",
       "         ('grateful', 'trust', 'bestowed'): 1,\n",
       "         ('trust', 'bestowed', 'mindful'): 1,\n",
       "         ('bestowed', 'mindful', 'sacrifice'): 1,\n",
       "         ('mindful', 'sacrifice', 'borne'): 1,\n",
       "         ('sacrifice', 'borne', 'ancestor'): 1,\n",
       "         ('borne', 'ancestor', 'I'): 1,\n",
       "         ('ancestor', 'I', 'thank'): 1,\n",
       "         ('I', 'thank', 'President'): 1,\n",
       "         ('thank', 'President', 'Bush'): 1,\n",
       "         ('President', 'Bush', 'service'): 1,\n",
       "         ('Bush', 'service', 'nation'): 1,\n",
       "         ('service', 'nation', 'well'): 1,\n",
       "         ('nation', 'well', 'generosity'): 1,\n",
       "         ('well', 'generosity', 'cooperation'): 1,\n",
       "         ('generosity', 'cooperation', 'shown'): 1,\n",
       "         ('cooperation', 'shown', 'throughout'): 1,\n",
       "         ('shown', 'throughout', 'transition'): 1,\n",
       "         ('throughout', 'transition', 'Americans'): 1,\n",
       "         ('transition', 'Americans', 'taken'): 1,\n",
       "         ('Americans', 'taken', 'presidential'): 1,\n",
       "         ('taken', 'presidential', 'oath'): 1,\n",
       "         ('presidential', 'oath', 'The'): 1,\n",
       "         ('oath', 'The', 'word'): 1,\n",
       "         ('The', 'word', 'spoken'): 1,\n",
       "         ('word', 'spoken', 'rising'): 1,\n",
       "         ('spoken', 'rising', 'tide'): 1,\n",
       "         ('rising', 'tide', 'prosperity'): 1,\n",
       "         ('tide', 'prosperity', 'still'): 1,\n",
       "         ('prosperity', 'still', 'water'): 1,\n",
       "         ('still', 'water', 'peace'): 1,\n",
       "         ('water', 'peace', 'Yet'): 1,\n",
       "         ('peace', 'Yet', 'every'): 1,\n",
       "         ('Yet', 'every', 'often'): 1,\n",
       "         ('every', 'often', 'oath'): 1,\n",
       "         ('often', 'oath', 'taken'): 1,\n",
       "         ('oath', 'taken', 'amidst'): 1,\n",
       "         ('taken', 'amidst', 'gathering'): 1,\n",
       "         ('amidst', 'gathering', 'cloud'): 1,\n",
       "         ('gathering', 'cloud', 'raging'): 1,\n",
       "         ('cloud', 'raging', 'storm'): 1,\n",
       "         ('raging', 'storm', 'At'): 1,\n",
       "         ('storm', 'At', 'moment'): 1,\n",
       "         ('At', 'moment', 'America'): 1,\n",
       "         ('moment', 'America', 'carried'): 1,\n",
       "         ('America', 'carried', 'simply'): 1,\n",
       "         ('carried', 'simply', 'skill'): 1,\n",
       "         ('simply', 'skill', 'vision'): 1,\n",
       "         ('skill', 'vision', 'high'): 1,\n",
       "         ('vision', 'high', 'office'): 1,\n",
       "         ('high', 'office', 'We'): 1,\n",
       "         ('office', 'We', 'People'): 1,\n",
       "         ('We', 'People', 'remained'): 1,\n",
       "         ('People', 'remained', 'faithful'): 1,\n",
       "         ('remained', 'faithful', 'ideal'): 1,\n",
       "         ('faithful', 'ideal', 'forbearers'): 1,\n",
       "         ('ideal', 'forbearers', 'true'): 1,\n",
       "         ('forbearers', 'true', 'founding'): 1,\n",
       "         ('true', 'founding', 'document'): 1,\n",
       "         ('founding', 'document', 'So'): 1,\n",
       "         ('document', 'So', 'So'): 1,\n",
       "         ('So', 'So', 'must'): 1,\n",
       "         ('So', 'must', 'generation'): 1,\n",
       "         ('must', 'generation', 'Americans'): 1,\n",
       "         ('generation', 'Americans', 'That'): 1,\n",
       "         ('Americans', 'That', 'midst'): 1,\n",
       "         ('That', 'midst', 'crisis'): 1,\n",
       "         ('midst', 'crisis', 'well'): 1,\n",
       "         ('crisis', 'well', 'understood'): 1,\n",
       "         ('well', 'understood', 'Our'): 1,\n",
       "         ('understood', 'Our', 'nation'): 1,\n",
       "         ('Our', 'nation', 'war'): 1,\n",
       "         ('nation', 'war', 'network'): 1,\n",
       "         ('war', 'network', 'violence'): 1,\n",
       "         ('network', 'violence', 'hatred'): 1,\n",
       "         ('violence', 'hatred', 'Our'): 1,\n",
       "         ('hatred', 'Our', 'economy'): 1,\n",
       "         ('Our', 'economy', 'badly'): 1,\n",
       "         ('economy', 'badly', 'weakened'): 1,\n",
       "         ('badly', 'weakened', 'consequence'): 1,\n",
       "         ('weakened', 'consequence', 'greed'): 1,\n",
       "         ('consequence', 'greed', 'irresponsibility'): 1,\n",
       "         ('greed', 'irresponsibility', 'part'): 1,\n",
       "         ('irresponsibility', 'part', 'also'): 1,\n",
       "         ('part', 'also', 'collective'): 1,\n",
       "         ('also', 'collective', 'failure'): 1,\n",
       "         ('collective', 'failure', 'make'): 1,\n",
       "         ('failure', 'make', 'hard'): 1,\n",
       "         ('make', 'hard', 'choice'): 1,\n",
       "         ('hard', 'choice', 'prepare'): 1,\n",
       "         ('choice', 'prepare', 'nation'): 1,\n",
       "         ('prepare', 'nation', 'new'): 1,\n",
       "         ('nation', 'new', 'age'): 1,\n",
       "         ('new', 'age', 'Homes'): 1,\n",
       "         ('age', 'Homes', 'lost'): 1,\n",
       "         ('Homes', 'lost', 'job'): 1,\n",
       "         ('lost', 'job', 'shed'): 1,\n",
       "         ('job', 'shed', 'business'): 1,\n",
       "         ('shed', 'business', 'shuttered'): 1,\n",
       "         ('business', 'shuttered', 'Our'): 1,\n",
       "         ('shuttered', 'Our', 'health'): 1,\n",
       "         ('Our', 'health', 'care'): 1,\n",
       "         ('health', 'care', 'costly'): 1,\n",
       "         ('care', 'costly', 'school'): 1,\n",
       "         ('costly', 'school', 'fail'): 1,\n",
       "         ('school', 'fail', 'many'): 1,\n",
       "         ('fail', 'many', 'day'): 1,\n",
       "         ('many', 'day', 'brings'): 1,\n",
       "         ('day', 'brings', 'evidence'): 1,\n",
       "         ('brings', 'evidence', 'way'): 1,\n",
       "         ('evidence', 'way', 'use'): 1,\n",
       "         ('way', 'use', 'energy'): 1,\n",
       "         ('use', 'energy', 'strengthen'): 1,\n",
       "         ('energy', 'strengthen', 'adversary'): 1,\n",
       "         ('strengthen', 'adversary', 'threaten'): 1,\n",
       "         ('adversary', 'threaten', 'planet'): 1,\n",
       "         ('threaten', 'planet', 'These'): 1,\n",
       "         ('planet', 'These', 'indicator'): 1,\n",
       "         ('These', 'indicator', 'crisis'): 1,\n",
       "         ('indicator', 'crisis', 'subject'): 1,\n",
       "         ('crisis', 'subject', 'data'): 1,\n",
       "         ('subject', 'data', 'statistic'): 1,\n",
       "         ('data', 'statistic', 'Less'): 1,\n",
       "         ('statistic', 'Less', 'measurable'): 1,\n",
       "         ('Less', 'measurable', 'le'): 1,\n",
       "         ('measurable', 'le', 'profound'): 1,\n",
       "         ('le', 'profound', 'sapping'): 1,\n",
       "         ('profound', 'sapping', 'confidence'): 1,\n",
       "         ('sapping', 'confidence', 'across'): 1,\n",
       "         ('confidence', 'across', 'land'): 1,\n",
       "         ('across', 'land', 'nagging'): 1,\n",
       "         ('land', 'nagging', 'fear'): 1,\n",
       "         ('nagging', 'fear', 'America'): 1,\n",
       "         ('fear', 'America', 'decline'): 1,\n",
       "         ('America', 'decline', 'inevitable'): 1,\n",
       "         ('decline', 'inevitable', 'next'): 1,\n",
       "         ('inevitable', 'next', 'generation'): 1,\n",
       "         ('next', 'generation', 'must'): 1,\n",
       "         ('generation', 'must', 'lower'): 1,\n",
       "         ('must', 'lower', 'sight'): 1,\n",
       "         ('lower', 'sight', 'Today'): 1,\n",
       "         ('sight', 'Today', 'I'): 1,\n",
       "         ('Today', 'I', 'say'): 1,\n",
       "         ('I', 'say', 'challenge'): 1,\n",
       "         ('say', 'challenge', 'face'): 1,\n",
       "         ('challenge', 'face', 'real'): 1,\n",
       "         ('face', 'real', 'They'): 1,\n",
       "         ('real', 'They', 'serious'): 1,\n",
       "         ('They', 'serious', 'many'): 1,\n",
       "         ('serious', 'many', 'They'): 1,\n",
       "         ('many', 'They', 'met'): 1,\n",
       "         ('They', 'met', 'easily'): 1,\n",
       "         ('met', 'easily', 'short'): 1,\n",
       "         ('easily', 'short', 'span'): 1,\n",
       "         ('short', 'span', 'time'): 1,\n",
       "         ('span', 'time', 'But'): 1,\n",
       "         ('time', 'But', 'know'): 1,\n",
       "         ('But', 'know', 'America'): 1,\n",
       "         ('know', 'America', 'met'): 1,\n",
       "         ('America', 'met', 'On'): 1,\n",
       "         ('met', 'On', 'day'): 1,\n",
       "         ('On', 'day', 'gather'): 1,\n",
       "         ('day', 'gather', 'chosen'): 1,\n",
       "         ('gather', 'chosen', 'hope'): 1,\n",
       "         ('chosen', 'hope', 'fear'): 1,\n",
       "         ('hope', 'fear', 'unity'): 1,\n",
       "         ('fear', 'unity', 'purpose'): 1,\n",
       "         ('unity', 'purpose', 'conflict'): 1,\n",
       "         ('purpose', 'conflict', 'discord'): 1,\n",
       "         ('conflict', 'discord', 'On'): 1,\n",
       "         ('discord', 'On', 'day'): 1,\n",
       "         ('On', 'day', 'come'): 1,\n",
       "         ('day', 'come', 'proclaim'): 1,\n",
       "         ('come', 'proclaim', 'end'): 1,\n",
       "         ('proclaim', 'end', 'petty'): 1,\n",
       "         ('end', 'petty', 'grievance'): 1,\n",
       "         ('petty', 'grievance', 'false'): 1,\n",
       "         ('grievance', 'false', 'promise'): 1,\n",
       "         ('false', 'promise', 'recrimination'): 1,\n",
       "         ('promise', 'recrimination', 'worn'): 1,\n",
       "         ('recrimination', 'worn', 'dogma'): 1,\n",
       "         ('worn', 'dogma', 'far'): 1,\n",
       "         ('dogma', 'far', 'long'): 1,\n",
       "         ('far', 'long', 'strangled'): 1,\n",
       "         ('long', 'strangled', 'politics'): 1})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRIGAMS\n",
    "Counter(list(ngrams(tokens,3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
